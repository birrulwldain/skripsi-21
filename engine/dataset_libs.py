# -*- coding: utf-8 -*-
"""dataset-libs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16lbFdn96oAC7aY5RARp-dpDvwRm53CYY

**REVISI , GUNAKAN PANJANG GELOMBANG RITZ DARI NIST, ATAU GUNAKAN PERHITUNGAN UNTUK PANJANG GELOMBANG SECARA TEORITIS DARI RUMUS LAMBDA = HC/DELTA E**
"""

import numpy as np
import pandas as pd
import json
import torch
import torch.nn.functional as F
from scipy.signal.windows import gaussian
import h5py
import re
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional
from google.colab import drive
import os
import ipywidgets as widgets
from IPython.display import display, clear_output

# Konfigurasi parameter simulasi
SIMULATION_CONFIG = {
    "resolution": 4096,
    "wl_range": (200, 900),
    "sigma": 0.1,
    "target_max_intensity": 0.8,
    "convolution_sigma": 0.1,
}

# Konstanta fisika
PHYSICAL_CONSTANTS = {
    "k_B": 8.617333262145e-5,  # eV/K
    "m_e": 9.1093837e-31,      # kg
    "h": 4.135667696e-15,      # eV·s
}

# Elemen target
BASE_ELEMENTS = ["Si", "Al", "Fe", "Ca", "Mg", "Na", "Ti", "Mn", "S", "Cl", "Cr", "Ni", "Cu"]
REQUIRED_ELEMENTS = [f"{elem}_{ion}" for elem in BASE_ELEMENTS for ion in [1, 2]]

def calculate_lte_electron_density(temp: float, delta_E: float) -> float:
    """
    Menghitung densitas elektron minimum untuk LTE berdasarkan suhu dan Delta E.

    Args:
        temp (float): Suhu dalam Kelvin.
        delta_E (float): Perbedaan energi transisi dalam eV.

    Returns:
        float: Densitas elektron minimum dalam cm^-3.
    """
    return 1.6e12 * (temp ** 0.5) * (delta_E ** 3)

class DataFetcher:
    def __init__(self, hdf_path: str):
        self.hdf_path = hdf_path
        self.delta_E_max = {}  # Menyimpan Delta E maksimum per elemen dan ion

    def get_nist_data(self, element: str, sp_num: int) -> Tuple[List[List], float]:
        """
        Mengambil data spektrum untuk elemen dan ion, dan menghitung Delta E maksimum.

        Args:
            element (str): Simbol elemen (misalnya, 'Si').
            sp_num (int): Nomor ionisasi (1 untuk netral, 2 untuk ion +1).

        Returns:
            Tuple[List[List], float]: Daftar baris data dan Delta E maksimum dalam eV.
        """
        try:
            with pd.HDFStore(self.hdf_path, mode='r') as store:
                df = store.get('nist_spectroscopy_data')
                filtered_df = df[(df['element'] == element) & (df['sp_num'] == sp_num)]
                required_columns = ['ritz_wl_air(nm)', 'Aki(s^-1)', 'Ek(eV)', 'Ei(eV)', 'g_i', 'g_k']
                if filtered_df.empty or not all(col in df.columns for col in required_columns):
                    print(f"No data found for {element}_{sp_num} in NIST dataset")
                    return [], 0.0
                filtered_df = filtered_df.dropna(subset=required_columns)

                # Konversi kolom ritz_wl_air(nm) ke float, ubah nilai non-numerik ke NaN
                filtered_df['ritz_wl_air(nm)'] = pd.to_numeric(
                    filtered_df['ritz_wl_air(nm)'], errors='coerce'
                )
                # Konversi Ek(eV) dan Ei(eV) ke float, hapus karakter non-numerik
                for col in ['Ek(eV)', 'Ei(eV)']:
                    filtered_df[col] = filtered_df[col].apply(
                        lambda x: float(re.sub(r'[^\d.-]', '', str(x))) if re.sub(r'[^\d.-]', '', str(x)) else None
                    )
                # Hapus baris dengan NaN di kolom penting
                filtered_df = filtered_df.dropna(subset=['ritz_wl_air(nm)', 'Ek(eV)', 'Ei(eV)'])

                # Filter berdasarkan rentang panjang gelombang
                filtered_df = filtered_df[
                    (filtered_df['ritz_wl_air(nm)'] >= SIMULATION_CONFIG["wl_range"][0]) &
                    (filtered_df['ritz_wl_air(nm)'] <= SIMULATION_CONFIG["wl_range"][1])
                ]
                # Hitung Delta E dalam eV
                filtered_df['delta_E'] = abs(filtered_df['Ek(eV)'] - filtered_df['Ei(eV)'])
                if not filtered_df.empty:
                    filtered_df = filtered_df.sort_values(by='Aki(s^-1)', ascending=False)
                    delta_E_max = filtered_df['delta_E'].max()
                    if pd.isna(delta_E_max):
                        delta_E_max = 0.0
                else:
                    delta_E_max = 0.0
                self.delta_E_max[f"{element}_{sp_num}"] = delta_E_max
                return filtered_df[required_columns + ['Acc']].values.tolist(), delta_E_max
        except Exception as e:
            print(f"Error fetching NIST data for {element}_{sp_num}: {str(e)}")
            return [], 0.0

class SpectrumSimulator:
    def __init__(
        self,
        nist_data: List[List],
        element: str,
        ion: int,
        temperature: float,
        ionization_energy: float,
        config: Dict = SIMULATION_CONFIG
    ):
        self.nist_data = nist_data
        self.element = element
        self.ion = ion
        self.temperature = temperature
        self.ionization_energy = ionization_energy
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.sigma = config["sigma"]
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.gaussian_cache = {}
        self.element_label = f"{element}_{ion}"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def partition_function(self, energy_levels: List[float], degeneracies: List[float]) -> float:
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return sum(g * np.exp(-E / (k_B * self.temperature)) for g, E in zip(degeneracies, energy_levels) if E is not None) or 1.0

    def calculate_intensity(self, energy: float, degeneracy: float, einstein_coeff: float, Z: float) -> float:
        k_B = PHYSICAL_CONSTANTS["k_B"]
        intensity = (degeneracy * einstein_coeff * np.exp(-energy / (k_B * self.temperature))) / Z
        return intensity

    def gaussian_profile(self, center: float) -> np.ndarray:
        if center not in self.gaussian_cache:
            x_tensor = torch.tensor(self.wavelengths, device=self.device, dtype=torch.float32)
            center_tensor = torch.tensor(center, device=self.device, dtype=torch.float32)
            sigma_tensor = torch.tensor(self.sigma, device=self.device, dtype=torch.float32)
            gaussian = torch.exp(-0.5 * ((x_tensor - center_tensor) / sigma_tensor) ** 2) / (sigma_tensor * torch.sqrt(torch.tensor(2 * np.pi, device=self.device)))
            self.gaussian_cache[center] = gaussian.cpu().numpy().astype(np.float32)
        return self.gaussian_cache[center]

    def simulate(self, atom_percentage: float = 1.0) -> Tuple[np.ndarray, np.ndarray]:
        spectrum = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
        levels = {}

        for data in self.nist_data:
            try:
                wl, Aki, Ek, Ei, gi, gk, _ = data
                if all(v is not None for v in [wl, Aki, Ek, Ei, gi, gk]):
                    Ek = float(Ek)
                    Ei = float(Ei)
                    if Ei not in levels:
                        levels[Ei] = float(gi)
                    if Ek not in levels:
                        levels[Ek] = float(gk)
            except (ValueError, TypeError):
                continue

        if not levels:
            return self.wavelengths, np.zeros(self.resolution, dtype=np.float32)

        energy_levels = list(levels.keys())
        degeneracies = list(levels.values())
        Z = self.partition_function(energy_levels, degeneracies)

        for data in self.nist_data:
            try:
                wl, Aki, Ek, Ei, gi, gk, _ = data
                if all(v is not None for v in [wl,  Aki, Ek, Ei, gi, gk]):
                    wl = float(wl)
                    Aki = float(Aki)
                    Ek = float(Ek)
                    intensity = self.calculate_intensity(Ek, float(gk), Aki, Z)
                    idx = np.searchsorted(self.wavelengths, wl)
                    if 0 <= idx < self.resolution:
                        gaussian_contrib = torch.tensor(
                            intensity * atom_percentage * self.gaussian_profile(wl),
                            device=self.device,
                            dtype=torch.float32
                        )
                        start_idx = max(0, idx - len(gaussian_contrib) // 2)
                        end_idx = min(self.resolution, start_idx + len(gaussian_contrib))
                        if start_idx < end_idx:
                            spectrum[start_idx:end_idx] += gaussian_contrib[:end_idx - start_idx]
            except (ValueError, TypeError):
                continue

        return self.wavelengths, spectrum.cpu().numpy()

class MixedSpectrumSimulator:
    def __init__(
        self,
        simulators: List[SpectrumSimulator],
        electron_density: float,
        delta_E_max: Dict[str, float],
        config: Dict = SIMULATION_CONFIG
    ):
        self.simulators = simulators
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.convolution_sigma = config["convolution_sigma"]
        self.electron_density = electron_density
        self.delta_E_max = delta_E_max
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def normalize_intensity(self, intensity: np.ndarray, target_max: float) -> np.ndarray:
        intensity_tensor = torch.tensor(intensity, device=self.device, dtype=torch.float32)
        max_intensity = torch.max(torch.abs(intensity_tensor))
        return intensity if max_intensity == 0 else (intensity_tensor / max_intensity * target_max).cpu().numpy()

    def convolve_spectrum(self, spectrum: np.ndarray, sigma_nm: float) -> np.ndarray:
        spectrum_tensor = torch.tensor(spectrum, device=self.device, dtype=torch.float32)
        wavelength_step = (self.wavelengths[-1] - self.wavelengths[0]) / (len(self.wavelengths) - 1)
        sigma_points = sigma_nm / wavelength_step
        kernel_size = int(6 * sigma_points) | 1
        kernel = torch.tensor(
            gaussian(kernel_size, sigma_points) / np.sum(gaussian(kernel_size, sigma_points)),
            device=self.device,
            dtype=torch.float32
        )
        kernel = kernel.unsqueeze(0).unsqueeze(0)
        spectrum_tensor = spectrum_tensor.unsqueeze(0).unsqueeze(0)
        convolved = F.conv1d(spectrum_tensor, kernel, padding=kernel_size//2).squeeze().cpu().numpy()
        return convolved.astype(np.float32)

    def saha_ratio(self, ion_energy: float, temp: float) -> float:
        k_B = PHYSICAL_CONSTANTS["k_B"]
        m_e = PHYSICAL_CONSTANTS["m_e"]
        h = PHYSICAL_CONSTANTS["h"]
        two_pi_me_kT_h2 = (2 * np.pi * m_e * (k_B * temp * 1.60217662e-19) / (h * 1.60217662e-19)**2) ** (3/2)
        two_pi_me_kT_h2 /= 1e6
        U_i = 1.0
        U_ip1 = 1.0
        saha_factor = (2 * U_ip1 / U_i) * two_pi_me_kT_h2 / self.electron_density
        return saha_factor * np.exp(-ion_energy / (k_B * temp))

    def validate_lte(self, temperature: float, selected_elements: List[Tuple[str, float]]) -> Tuple[float, float]:
        """
        Memvalidasi apakah electron_density memenuhi LTE dan mengembalikan delta_E_max serta n_e_min.

        Args:
            temperature (float): Suhu dalam Kelvin.
            selected_elements (List[Tuple[str, float]]): Daftar elemen dan persentasenya.

        Returns:
            Tuple[float, float]: delta_E_max dan n_e_min yang dihitung.
        """
        delta_E_values = []
        for base_elem, _ in selected_elements:
            for ion in [1, 2]:
                delta_E = self.delta_E_max.get(f"{base_elem}_{ion}", 0.0)
                if delta_E > 0.0:
                    delta_E_values.append(delta_E)
        delta_E_max = max(delta_E_values) if delta_E_values else 4.0
        n_e_min = calculate_lte_electron_density(temperature, delta_E_max)
        return delta_E_max, n_e_min

    def doppler_width(self, center: float) -> float:
      k_B = 1.380649e-23  # J/K (konstanta Boltzmann dalam SI)
      c = PHYSICAL_CONSTANTS["c"]
      m = self.atomic_mass
      T = self.temperature
      lambda_0 = center * 1e-9  # nm ke m
      sigma_D = (lambda_0 / c) * np.sqrt((2 * k_B * T) / m)
      return sigma_D * 1e9  # Kembali ke nm
    def generate_spectrum(
        self,
        selected_elements: List[Tuple[str, float]],
        temperature: float
    ) -> Tuple[np.ndarray, np.ndarray, Dict]:
        mixed_spectrum = np.zeros(self.resolution, dtype=np.float32)
        atom_percentages_dict = {}
        total_target_percentage = 0.0

        # Validasi LTE
        delta_E_max, n_e_min = self.validate_lte(temperature, selected_elements)
        if self.electron_density < n_e_min:
            print(f"Warning: Electron density {self.electron_density:.1e} cm^-3 is below LTE requirement "
                  f"({n_e_min:.1e} cm^-3) for T={temperature:.0f} K, ΔE_max={delta_E_max:.2f} eV")
            self.electron_density = n_e_min
            print(f"Using n_e = {self.electron_density:.1e} cm^-3 to satisfy LTE")

        for base_elem, percentage in selected_elements:
            ion_energy = ionization_energies.get(f"{base_elem} I", 0.0)
            if ion_energy == 0.0:
                print(f"Warning: No ionization energy for {base_elem} I")
                continue
            saha_ratio = self.saha_ratio(ion_energy, temperature)
            neutral_fraction = 1 / (1 + saha_ratio)
            ion_fraction = saha_ratio / (1 + saha_ratio)
            neutral_percentage = percentage * neutral_fraction / 100.0
            ion_percentage = percentage * ion_fraction / 100.0
            atom_percentages_dict[f"{base_elem}_1"] = neutral_percentage
            atom_percentages_dict[f"{base_elem}_2"] = ion_percentage
            total_target_percentage += percentage

        if abs(total_target_percentage - 100.0) > 1e-6:
            raise ValueError(f"Total percentage ({total_target_percentage:.1f}%) must be exactly 100%")

        for simulator in self.simulators:
            elem_label = f"{simulator.element}_{simulator.ion}"
            if elem_label in atom_percentages_dict:
                simulator.temperature = temperature  # Perbarui suhu simulator
                wavelengths, spectrum = simulator.simulate(atom_percentages_dict[elem_label])
                mixed_spectrum += spectrum

        if np.max(mixed_spectrum) == 0:
            print(f"Warning: No spectrum generated for temperature {temperature}")
            return self.wavelengths, np.zeros(self.resolution, dtype=np.float32), atom_percentages_dict

        convolved_spectrum = self.convolve_spectrum(mixed_spectrum, self.convolution_sigma)
        normalized_spectrum = self.normalize_intensity(convolved_spectrum, SIMULATION_CONFIG["target_max_intensity"])
        atom_percentages_dict['temperature'] = float(temperature)
        atom_percentages_dict['electron_density'] = float(self.electron_density)
        atom_percentages_dict['delta_E_max'] = float(delta_E_max)
        atom_percentages_dict['n_e_min'] = float(n_e_min)
        return self.wavelengths, normalized_spectrum, {k: float(v * 100) if isinstance(v, (int, float)) and k not in ['temperature', 'electron_density', 'delta_E_max', 'n_e_min'] else float(v) for k, v in atom_percentages_dict.items()}

def plot_spectrum(wavelengths: np.ndarray, spectrum: np.ndarray, temperature: float, electron_density: float, atom_percentages: Dict):
    plt.style.use('default')
    plt.rcParams.update({
        'font.family': 'sans-serif',
        'font.sans-serif': ['DejaVu Sans', 'Helvetica', 'Arial'],
        'font.size': 14,
        'axes.labelsize': 16,
        'axes.titlesize': 18,
        'xtick.labelsize': 12,
        'ytick.labelsize': 12,
        'legend.fontsize': 12,
        'lines.linewidth': 1.5,
        'axes.linewidth': 1.2,
        'xtick.major.size': 5,
        'ytick.major.size': 5,
        'xtick.minor.size': 3,
        'ytick.minor.size': 3,
        'figure.dpi': 300,
        'savefig.dpi': 300,
        'axes.facecolor': 'white',
        'figure.facecolor': 'white',
    })

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(wavelengths, spectrum, color='navy', label='Simulated Spectrum', linewidth=1.5)
    ax.set_xlabel('Wavelength (nm)', fontsize=16, labelpad=10)
    ax.set_ylabel('Normalized Intensity (a.u.)', fontsize=16, labelpad=10)
    ax.set_title(f'Simulated Stellar Spectrum\n$T = {temperature:.0f}$ K, $n_e = {electron_density:.1e}$ cm$^{-3}$',
                 fontsize=18, pad=15)
    ax.set_xlim(wavelengths[0], wavelengths[-1])
    ax.set_ylim(0, max(spectrum) * 1.1 if max(spectrum) > 0 else 1.0)
    ax.grid(True, which='major', linestyle='--', linewidth=0.5, alpha=0.7, color='gray')
    ax.grid(True, which='minor', linestyle=':', linewidth=0.3, alpha=0.5, color='gray')
    ax.minorticks_on()
    ax.legend(loc='upper right', frameon=True, edgecolor='black', fontsize=12)
    comp_text = 'Composition:\n' + '\n'.join(
        [f'{elem}: {perc:.2f}%' for elem, perc in atom_percentages.items() if elem not in ['temperature', 'electron_density', 'delta_E_max', 'n_e_min']] +
        [f'ΔE_max: {atom_percentages["delta_E_max"]:.2f} eV', f'n_e_min: {atom_percentages["n_e_min"]:.1e} cm^-3']
    )
    ax.text(0.02, 0.98, comp_text, transform=ax.transAxes, fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='black'))
    plt.tight_layout()
    plt.savefig('spectrum_plot.png', dpi=300, bbox_inches='tight', format='png')
    plt.show()
    plt.close()

def create_composition_form():
    element_options = [(elem, elem) for elem in BASE_ELEMENTS]
    composition_widgets = []
    total_percentage_label = widgets.Label(value="Total Percentage: 0.0%")

    def add_element_row(_=None):
        element_dropdown = widgets.Dropdown(
            options=element_options,
            description="Element:",
            layout={'width': '300px'}
        )
        percentage_input = widgets.FloatText(
            value=0.0,
            description="Percentage (%):",
            layout={'width': '200px'}
        )
        remove_button = widgets.Button(description="Remove", button_style='danger')

        def update_total(_=None):
            total = sum(w[1].value for w in composition_widgets)
            total_percentage_label.value = f"Total Percentage: {total:.1f}%"

        def remove_row(_):
            composition_widgets.remove((element_dropdown, percentage_input, remove_button))
            hbox.children = [w for row in composition_widgets for w in row] + [add_button, total_percentage_label]
            update_total()

        percentage_input.observe(update_total, names='value')
        remove_button.on_click(remove_row)
        composition_widgets.append((element_dropdown, percentage_input, remove_button))
        hbox.children = [w for row in composition_widgets for w in row] + [add_button, total_percentage_label]
        update_total()

    add_button = widgets.Button(description="Add Element", button_style='success')
    add_button.on_click(add_element_row)
    hbox = widgets.VBox([add_button, total_percentage_label])
    display(hbox)
    add_element_row()

    submit_button = widgets.Button(description="Generate Spectrum", button_style='primary')
    output = widgets.Output()

    def on_submit(_):
        with output:
            clear_output()
            selected_elements = []
            total_percentage = 0.0
            for element_dropdown, percentage_input, _ in composition_widgets:
                elem = element_dropdown.value
                percentage = percentage_input.value
                if percentage < 0:
                    print(f"Error: Percentage for {elem} must be non-negative")
                    return
                selected_elements.append((elem, percentage))
                total_percentage += percentage

            if abs(total_percentage - 100.0) > 1e-6:
                print(f"Error: Total percentage ({total_percentage:.1f}%) must be exactly 100%")
                return
            if not selected_elements:
                print("Error: No elements selected")
                return

            # Akses nist_path untuk menghitung delta_E_max
            data_dir = "/content/drive/MyDrive/libs_lstm/data/raw/HDF5"
            nist_path = os.path.join(data_dir, "data_nist.h5")
            fetcher = DataFetcher(nist_path)
            delta_E_max_dict = {}
            for elem, _ in selected_elements:
                for ion in [1, 2]:
                    _, delta_E = fetcher.get_nist_data(elem, ion)
                    delta_E_max_dict[f"{elem}_{ion}"] = delta_E
            delta_E_values = [d for d in delta_E_max_dict.values() if d > 0.0]
            delta_E_max = max(delta_E_values) if delta_E_values else 4.0
            n_e_min = calculate_lte_electron_density(temperature_input.value, delta_E_max)

            # Buat opsi n_e sebagai kelipatan 10
            base_n_e = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
            exponents = [15, 16, 17, 18]
            possible_n_e = []
            for exp in exponents:
                for base in base_n_e:
                    n_e = base * (10 ** exp)
                    if n_e >= n_e_min:
                        possible_n_e.append(n_e)
            if not possible_n_e:
                possible_n_e = [n_e_min]
                print(f"Warning: No predefined n_e satisfies LTE. Using n_e_min = {n_e_min:.1e} cm^-3")

            # Pilih n_e dari dropdown, default ke opsi pertama jika tidak valid
            selected_n_e = electron_density_input.value
            if selected_n_e not in possible_n_e:
                selected_n_e = possible_n_e[0]
                print(f"Adjusted n_e to {selected_n_e:.1e} cm^-3 to satisfy LTE")

            run_simulation(selected_elements, temperature_input.value, selected_n_e)

    submit_button.on_click(on_submit)
    temperature_input = widgets.FloatText(
        value=10000,
        description="Temperature (K):",
        layout={'width': '300px'}
    )
    # Gunakan Dropdown untuk n_e
    electron_density_input = widgets.Dropdown(
        options=[1e15, 2e15, 3e15, 4e15, 5e15, 6e15, 7e15, 8e15, 9e15, 1e16, 2e16, 3e16, 4e16, 5e16, 6e16, 7e16, 8e16, 9e16, 1e17, 2e17, 3e17, 4e17, 5e17, 6e17, 7e17, 8e17, 9e17, 1e18],
        value=1e17,
        description="Electron Density (cm^-3):",
        layout={'width': '300px'}
    )
    display(temperature_input, electron_density_input, submit_button, output)
    return composition_widgets, temperature_input, electron_density_input

def run_simulation(selected_elements: List[Tuple[str, float]], temperature: float, electron_density: float):
    mountpoint = '/content/drive'
    try:
        drive.flush_and_unmount()
    except ValueError:
        pass
    os.makedirs(mountpoint, exist_ok=True)
    drive.mount(mountpoint, force_remount=True)

    data_dir = "/content/drive/MyDrive/libs_lstm/data/raw/HDF5"
    nist_path = os.path.join(data_dir, "nist_data(1).h5")
    atomic_data_path = os.path.join(data_dir, "atomic_data1.h5")
    json_map_path = "/content/drive/MyDrive/libs_lstm/data/processed/element_map.json"

    global ionization_energies
    ionization_energies = {}
    with h5py.File(atomic_data_path, 'r') as f:
        dset = f['elements']
        columns = dset.attrs['columns']
        data = [
            [item[0], item[1].decode('utf-8'), item[2].decode('utf-8'), item[3].decode('utf-8'),
             item[4].decode('utf-8'), item[5], item[6].decode('utf-8')]
            for item in dset[:]
        ]
        df_ionization = pd.DataFrame(data, columns=columns)
        for _, row in df_ionization.iterrows():
            ionization_energies[row["Sp. Name"]] = float(row["Ionization Energy (eV)"])

    with open(json_map_path, 'r') as f:
        element_map = json.load(f)

    fetcher = DataFetcher(nist_path)
    simulators = []
    delta_E_max_dict = {}
    for elem, percentage in selected_elements:
        for ion in [1, 2]:
            nist_data, delta_E = fetcher.get_nist_data(elem, ion)
            if not nist_data:
                print(f"No NIST data for {elem}_{ion}")
                continue
            delta_E_max_dict[f"{elem}_{ion}"] = delta_E
            ion_energy = ionization_energies.get(f"{elem} {'I' if ion == 1 else 'II'}", 0.0)
            simulator = SpectrumSimulator(nist_data, elem, ion, temperature, ion_energy)
            simulators.append(simulator)

    if not simulators:
        print("No valid simulators. Exiting.")
        return

    mixed_simulator = MixedSpectrumSimulator(simulators, electron_density, delta_E_max_dict)
    wavelengths, spectrum, atom_percentages = mixed_simulator.generate_spectrum(selected_elements, temperature)

    print(f"Spectrum generated with:")
    print(f"Temperature: {temperature:.0f} K")
    print(f"Electron Density: {atom_percentages['electron_density']:.1e} cm^-3")
    print(f"ΔE_max: {atom_percentages['delta_E_max']:.2f} eV")
    print(f"n_e_min: {atom_percentages['n_e_min']:.1e} cm^-3")
    print("Atomic Composition (after Saha adjustment):")
    for elem, percentage in atom_percentages.items():
        if elem not in ['temperature', 'electron_density', 'delta_E_max', 'n_e_min']:
            print(f"  {elem}: {percentage:.2f}%")

    plot_spectrum(wavelengths, spectrum, temperature, atom_percentages['electron_density'], atom_percentages)

def main():
    create_composition_form()

if __name__ == "__main__":
    main()

#Versi Stabil Multi-elemen Acak
import os
import numpy as np
import pandas as pd
import json
import torch
import torch.nn.functional as F
from scipy.signal.windows import gaussian
import h5py
from tqdm import tqdm
import shutil
from typing import List, Dict, Tuple, Optional
from google.colab import drive

# Konfigurasi simulasi
SIMULATION_CONFIG = {
    "resolution": 4096,
    "wl_range": (200, 900),
    "sigma": 0.1,  # nm, untuk pelebaran Gaussian
    "target_max_intensity": 0.8,
    "convolution_sigma": 0.1,  # nm, untuk konvolusi spektrum
    "num_samples": 500,
    "temperature_range": [6000, 8000, 10000, 12000, 14000, 15000],  # K
    "electron_density_range": np.logspace(15, 17.7, 10).tolist(),  # cm^-3, dari 1e15 hingga 5e17
}

# Konstanta fisika
PHYSICAL_CONSTANTS = {
    "k_B": 8.617333262145e-5,  # eV/K
    "m_e": 9.1093837e-31,      # kg
    "h": 4.135667696e-15,      # eV·s
}

# Elemen dan ion
BASE_ELEMENTS = ["Si", "Al", "Fe", "Ca"]
REQUIRED_ELEMENTS = [f"{elem}_{ion}" for elem in BASE_ELEMENTS for ion in [1, 2]]

def calculate_lte_electron_density(temp: float, delta_E: float) -> float:
    """Menghitung densitas elektron minimum untuk LTE berdasarkan suhu dan Delta E.

    Args:
        temp: Suhu dalam Kelvin.
        delta_E: Perbedaan energi transisi dalam eV.

    Returns:
        Densitas elektron minimum dalam cm^-3.
    """
    return 1.6e12 * (temp ** 0.5) * (delta_E ** 3)

class DataFetcher:
    """Mengambil data spektrum dari file HDF5 NIST untuk elemen dan ion tertentu."""

    def __init__(self, hdf_path: str):
        self.hdf_path = hdf_path
        self.delta_E_max = {}  # Cache Delta E maksimum per elemen dan ion

    def get_nist_data(self, element: str, sp_num: int) -> Tuple[List[List], float]:
        """Mengambil data spektrum untuk elemen dan ion, serta menghitung Delta E maksimum.

        Args:
            element: Simbol elemen (misalnya, 'Si').
            sp_num: Nomor ionisasi (1 untuk netral, 2 untuk ion +1).

        Returns:
            Tuple berisi daftar baris data (wavelength,  Aki, Ek, Ei, g_i, g_k, Acc)
            dan Delta E maksimum dalam eV. Mengembalikan ([], 0.0) jika gagal.
        """
        try:
            with pd.HDFStore(self.hdf_path, mode='r') as store:
                df = store.get('spectrum_data')
                filtered_df = df[(df['element'] == element) & (df['sp_num'] == sp_num)]
                required_columns = ['ritz_wl_air(nm))', 'Aki(s^-1)', 'Ek(eV)', 'Ei(eV)', 'g_i', 'g_k']

                if filtered_df.empty or not all(col in df.columns for col in required_columns):
                    print(f"No data found for {element}_{sp_num} in NIST dataset")
                    return [], 0.0

                filtered_df = filtered_df.dropna(subset=required_columns)

                # Konversi kolom ke numerik
                filtered_df['ritz_wl_air(nm)'] = pd.to_numeric(filtered_df['ritz_wl_air(nm)'], errors='coerce')
                for col in ['Ek(eV)', 'Ei(eV)', 'Aki(s^-1)', 'g_i', 'g_k']:
                    filtered_df[col] = pd.to_numeric(
                        filtered_df[col].apply(lambda x: re.sub(r'[^\d.-]', '', str(x)) if re.sub(r'[^\d.-]', '', str(x)) else None),
                        errors='coerce'
                    )

                filtered_df = filtered_df.dropna(subset=['ritz_wl_air(nm)', 'Ek(eV)', 'Ei(eV)', 'Aki(s^-1)', 'g_i', 'g_k'])

                # Filter rentang panjang gelombang
                filtered_df = filtered_df[
                    (filtered_df['ritz_wl_air(nm)'] >= SIMULATION_CONFIG["wl_range"][0]) &
                    (filtered_df['ritz_wl_air(nm)'] <= SIMULATION_CONFIG["wl_range"][1])
                ]

                # Hitung Delta E (eV)
                filtered_df['delta_E'] = abs(filtered_df['Ek(eV)'] - filtered_df['Ei(eV)'])
                if filtered_df.empty:
                    print(f"No valid transitions for {element}_{sp_num} in wavelength range")
                    return [], 0.0

                filtered_df = filtered_df.sort_values(by='Aki(s^-1)', ascending=False)
                delta_E_max = filtered_df['delta_E'].max()
                delta_E_max = 0.0 if pd.isna(delta_E_max) else delta_E_max
                self.delta_E_max[f"{element}_{sp_num}"] = delta_E_max

                return filtered_df[required_columns + ['Acc']].values.tolist(), delta_E_max
        except Exception as e:
            print(f"Error fetching NIST data for {element}_{sp_num}: {str(e)}")
            return [], 0.0

class SpectrumSimulator:
    """Mensimulasikan spektrum emisi untuk satu elemen dan ion pada berbagai suhu."""

    def __init__(
        self,
        nist_data: List[List],
        element: str,
        ion: int,
        temperatures: List[float],
        ionization_energy: float,
        config: Dict = SIMULATION_CONFIG
    ):
        self.nist_data = nist_data
        self.element = element
        self.ion = ion
        self.temperatures = temperatures
        self.ionization_energy = ionization_energy
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.sigma = config["sigma"]
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.gaussian_cache = {}
        self.element_label = f"{element}_{ion}"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def partition_function(self, energy_levels: List[float], degeneracies: List[float], temperature: float) -> float:
        """Menghitung fungsi partisi untuk tingkat energi pada suhu tertentu."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return sum(g * np.exp(-E / (k_B * temperature)) for g, E in zip(degeneracies, energy_levels) if E is not None) or 1.0

    def calculate_intensity(self, temperature: float, energy: float, degeneracy: float, einstein_coeff: float, Z: float) -> float:
        """Menghitung intensitas garis spektral dalam kondisi LTE."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return (degeneracy * einstein_coeff * np.exp(-energy / (k_B * temperature))) / Z

    def gaussian_profile(self, center: float) -> np.ndarray:
        """Menghasilkan profil Gaussian untuk panjang gelombang pusat."""
        if center not in self.gaussian_cache:
            x_tensor = torch.tensor(self.wavelengths, device=self.device, dtype=torch.float32)
            center_tensor = torch.tensor(center, device=self.device, dtype=torch.float32)
            sigma_tensor = torch.tensor(self.sigma, device=self.device, dtype=torch.float32)
            gaussian = torch.exp(-0.5 * ((x_tensor - center_tensor) / sigma_tensor) ** 2) / (sigma_tensor * torch.sqrt(torch.tensor(2 * np.pi)))
            self.gaussian_cache[center] = gaussian.cpu().numpy().astype(np.float32)
        return self.gaussian_cache[center]

    def simulate(self, atom_percentage: float = 1.0) -> Tuple[np.ndarray, List[np.ndarray], List[List[int]], List[List[int]], List[float], List[List[Dict]], List[np.ndarray]]:
        """Mensimulasikan spektrum untuk semua suhu yang diberikan.

        Returns:
            Tuple berisi panjang gelombang, spektrum, indeks puncak, label puncak,
            suhu, data intensitas, dan kontribusi elemen.
        """
        spectra, peak_indices, peak_labels, temperatures, intensity_data, contributions = [], [], [], [], [], []
        levels = {}

        for data in self.nist_data:
            try:
                wl, Aki, Ek, Ei, gi, gk, _ = data
                if all(v is not None for v in [wl, Aki, Ek, Ei, gi, gk]):
                    Ek = float(Ek)
                    Ei = float(Ei)
                    if Ei not in levels:
                        levels[Ei] = float(gi)
                    if Ek not in levels:
                        levels[Ek] = float(gk)
            except (ValueError, TypeError):
                continue

        if not levels:
            print(f"No valid energy levels for {self.element_label}")
            return self.wavelengths, [], [], [], [], [], []

        energy_levels = list(levels.keys())
        degeneracies = list(levels.values())

        for temp in self.temperatures:
            Z = self.partition_function(energy_levels, degeneracies, temp)
            intensities = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
            element_contributions = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
            peak_idx, peak_label, temp_intensity_data = [], [], []

            for data in self.nist_data:
                try:
                    wl,  Aki, Ek, Ei, gi, gk, _ = data
                    if all(v is not None for v in [wl,  Aki, Ek, Ei, gi, gk]):
                        wl = float(wl)
                        AKi = float(AKi)
                        Ek = float(Ek)
                        intensity = self.calculate_intensity(temp, Ek, float(gk),  Aki, Z)
                        idx = np.searchsorted(self.wavelengths, wl)
                        if 0 <= idx < self.resolution:
                            gaussian_contrib = torch.tensor(
                                intensity * atom_percentage * self.gaussian_profile(wl),
                                device=self.device,
                                dtype=torch.float32
                            )
                            start_idx = max(0, idx - len(gaussian_contrib) // 2)
                            end_idx = min(self.resolution, start_idx + len(gaussian_contrib))
                            if start_idx < end_idx:
                                intensities[start_idx:end_idx] += gaussian_contrib[:end_idx - start_idx]
                                element_contributions[start_idx:end_idx] += gaussian_contrib[:end_idx - start_idx]
                            temp_intensity_data.append({
                                'wavelength': wl,
                                'intensity': intensity * atom_percentage,
                                'element_label': self.element_label,
                                'index': idx
                            })
                            class_idx = REQUIRED_ELEMENTS.index(self.element_label) if self.element_label in REQUIRED_ELEMENTS else len(REQUIRED_ELEMENTS)
                            peak_idx.append(idx)
                            peak_label.append(class_idx)
                except (ValueError, TypeError):
                    continue

            spectra.append(intensities.cpu().numpy())
            peak_indices.append(peak_idx)
            peak_labels.append(peak_label)
            temperatures.append(temp)
            intensity_data.append(temp_intensity_data)
            contributions.append(element_contributions.cpu().numpy())

        return self.wavelengths, spectra, peak_indices, peak_labels, temperatures, intensity_data, contributions

class MixedSpectrumSimulator:
    """Menggabungkan spektrum dari beberapa elemen berdasarkan proporsi atom."""

    def __init__(self, simulators: List[SpectrumSimulator], config: Dict, delta_E_max: Dict[str, float]):
        self.simulators = simulators
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.convolution_sigma = config["convolution_sigma"]
        self.electron_density_range = config["electron_density_range"]
        self.delta_E_max = delta_E_max
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.intensity_threshold = 0.01

    def normalize_intensity(self, intensity: np.ndarray, target_max: float) -> np.ndarray:
        """Menormalkan intensitas spektrum ke nilai maksimum target."""
        intensity_tensor = torch.tensor(intensity, device=self.device, dtype=torch.float32)
        max_intensity = torch.max(torch.abs(intensity_tensor))
        return intensity if max_intensity == 0 else (intensity_tensor / max_intensity * target_max).cpu().numpy()

    def convolve_spectrum(self, spectrum: np.ndarray, sigma_nm: float) -> np.ndarray:
        """Mengonvolusi spektrum dengan kernel Gaussian."""
        spectrum_tensor = torch.tensor(spectrum, device=self.device, dtype=torch.float32)
        wavelength_step = (self.wavelengths[-1] - self.wavelengths[0]) / (len(self.wavelengths) - 1)
        sigma_points = sigma_nm / wavelength_step
        kernel_size = int(6 * sigma_points) | 1
        kernel = torch.tensor(
            gaussian(kernel_size, sigma_points) / np.sum(gaussian(kernel_size, sigma_points)),
            device=self.device,
            dtype=torch.float32
        )
        kernel = kernel.unsqueeze(0).unsqueeze(0)
        spectrum_tensor = spectrum_tensor.unsqueeze(0).unsqueeze(0)
        convolved = F.conv1d(spectrum_tensor, kernel, padding=kernel_size//2).squeeze().cpu().numpy()
        return convolved.astype(np.float32)

    def saha_ratio(self, ion_energy: float, temp: float, electron_density: float) -> float:
        """Menghitung rasio Saha untuk ionisasi."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        m_e = PHYSICAL_CONSTANTS["m_e"]
        h = PHYSICAL_CONSTANTS["h"]
        two_pi_me_kT_h2 = (2 * np.pi * m_e * (k_B * temp * 1.60217662e-19) / (h * 1.60217662e-19)**2) ** (3/2)
        two_pi_me_kT_h2 /= 1e6
        U_i = 1.0
        U_ip1 = 1.0
        saha_factor = (2 * U_ip1 / U_i) * two_pi_me_kT_h2 / electron_density
        return saha_factor * np.exp(-ion_energy / (k_B * temp))

    def generate_sample(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray], Optional[Dict]]:
        """Menghasilkan satu sampel spektrum campuran."""
        temp = np.random.choice(SIMULATION_CONFIG["temperature_range"])

        # Pilih elemen secara acak
        num_target_elements = 4
        selected_base_elements = np.random.choice(BASE_ELEMENTS, num_target_elements, replace=False)
        selected_pairs = [(elem, f"{elem}_1", f"{elem}_2") for elem in selected_base_elements]

        # Hitung Delta E maksimum
        delta_E_values = []
        for base_elem, elem_neutral, elem_ion in selected_pairs:
            for elem in [elem_neutral, elem_ion]:
                delta_E = self.delta_E_max.get(elem, 0.0)
                if delta_E > 0.0:
                    delta_E_values.append(delta_E)
        delta_E_max = max(delta_E_values) if delta_E_values else 4.0

        # Pilih densitas elektron yang memenuhi LTE
        n_e_min = calculate_lte_electron_density(temp, delta_E_max)
        valid_n_e = [n_e for n_e in self.electron_density_range if n_e >= n_e_min]
        if not valid_n_e:
            print(f"Warning: No valid n_e for T={temp} K, delta_E={delta_E_max:.2f} eV, n_e_min={n_e_min:.2e} cm^-3")
            electron_density = max(self.electron_density_range)
        else:
            electron_density = np.random.choice(valid_n_e)

        # Peringatan absorpsi diri untuk densitas tinggi/suhu rendah
        if electron_density > 5e16 and temp < 8000:
            print(f"Warning: High n_e ({electron_density:.2e} cm^-3) and low T ({temp} K) may cause self-absorption.")

        # Hitung fraksi atom menggunakan rasio Saha
        atom_percentages_dict = {}
        total_target_percentage = 0.0
        for base_elem, elem_neutral, elem_ion in selected_pairs:
            ion_energy = ionization_energies.get(f"{base_elem} I", 0.0)
            if ion_energy == 0.0:
                print(f"Warning: No ionization energy for {base_elem} I")
                continue
            saha_ratio = self.saha_ratio(ion_energy, temp, electron_density)
            total_percentage = np.random.uniform(5, 20)
            fraction_neutral = 1 / (1 + saha_ratio)
            fraction_ion = saha_ratio / (1 + saha_ratio)
            percentage_neutral = total_percentage * fraction_neutral
            percentage_ion = total_percentage * fraction_ion
            atom_percentages_dict[elem_neutral] = percentage_neutral / 100.0
            atom_percentages_dict[elem_ion] = percentage_ion / 100.0
            total_target_percentage += total_percentage

        # Skala persentase agar total 100%
        if total_target_percentage != 0:
            scaling_factor = 100.0 / total_target_percentage
            for key in atom_percentages_dict:
                atom_percentages_dict[key] *= scaling_factor
        else:
            return None, None, None, None

        atom_percentages_dict['temperature'] = float(temp)
        atom_percentages_dict['electron_density'] = float(electron_density)
        atom_percentages_dict['delta_E_max'] = float(delta_E_max)

        selected_target_elements = [k for k in atom_percentages_dict.keys() if k not in ['temperature', 'electron_density', 'delta_E_max']]
        atom_percentages = np.array([atom_percentages_dict[elem] for elem in selected_target_elements], dtype=np.float32)
        selected_simulators = [sim for sim in self.simulators if f"{sim.element}_{sim.ion}" in selected_target_elements]

        if not selected_simulators:
            print(f"Warning: No valid simulators for {selected_target_elements}")
            return None, None, None, None

        mixed_spectrum = np.zeros(self.resolution, dtype=np.float32)
        element_contributions = np.zeros((len(selected_simulators), self.resolution), dtype=np.float32)

        for sim_idx, simulator in enumerate(selected_simulators):
            idx = selected_target_elements.index(f"{simulator.element}_{simulator.ion}")
            atom_percentage = atom_percentages[idx]
            wavelengths, element_spectra, _, _, temps, _, contributions = simulator.simulate(atom_percentage)
            for spectrum, t, contrib in zip(element_spectra, temps, contributions):
                if t == temp:
                    mixed_spectrum += spectrum
                    element_contributions[sim_idx, :] = contrib
                    break

        if np.max(mixed_spectrum) == 0:
            print(f"Warning: No spectrum generated for T={temp} K")
            return None, None, None, None

        convolved_spectrum = self.convolve_spectrum(mixed_spectrum, self.convolution_sigma)
        normalized_spectrum = self.normalize_intensity(convolved_spectrum, SIMULATION_CONFIG["target_max_intensity"])

        labels = np.zeros(self.resolution, dtype=np.int32)
        contributions = np.zeros((self.resolution, len(REQUIRED_ELEMENTS)), dtype=np.float32)

        for idx in range(self.resolution):
            contributions_at_point = element_contributions[:, idx]
            total_intensity = np.sum(contributions_at_point)
            if total_intensity >= self.intensity_threshold:
                dominant_sim_idx = np.argmax(contributions_at_point)
                dominant_element = selected_simulators[dominant_sim_idx].element_label
                if dominant_element in REQUIRED_ELEMENTS:
                    dominant_label = REQUIRED_ELEMENTS.index(dominant_element)
                    labels[idx] = dominant_label + 1
                    contributions[idx, dominant_label] = normalized_spectrum[idx]

        atom_percentages_dict = {
            k: float(v * 100) if k not in ['temperature', 'electron_density', 'delta_E_max'] else float(v)
            for k, v in atom_percentages_dict.items()
        }
        return self.wavelengths, normalized_spectrum, labels, atom_percentages_dict

def generate_dataset(
    num_samples: int,
    simulators: List[SpectrumSimulator],
    delta_E_max: Dict[str, float],
    processed_dir: str,
    drive_processed_dir: str
) -> None:
    """Menghasilkan dan menyimpan dataset spektrum ke file HDF5."""
    mixed_simulator = MixedSpectrumSimulator(simulators, SIMULATION_CONFIG, delta_E_max)
    spectra_list, labels_list, wavelengths_list, atom_percentages_list = [], [], [], []

    for _ in tqdm(range(num_samples), desc="Generating dataset"):
        result = mixed_simulator.generate_sample()
        if result[0] is not None:
            wavelengths, spectrum, labels, atom_percentages = result
            spectra_list.append(spectrum)
            labels_list.append(labels)
            wavelengths_list.append(wavelengths)
            atom_percentages_list.append(atom_percentages)

    if not spectra_list:
        raise ValueError("No valid samples generated. Check NIST data or simulator configuration.")

    spectra_array = np.array(spectra_list, dtype=np.float32)
    labels_array = np.array(labels_list, dtype=np.int32)
    wavelengths_array = np.array(wavelengths_list, dtype=np.float32)
    atom_percentages_array = [json.dumps(d).encode('utf-8') for d in atom_percentages_list]

    # Pisahkan data menjadi train, validation, test
    actual_samples = len(spectra_list)
    num_train = int(0.7 * actual_samples)
    num_val = int(0.15 * actual_samples)
    num_test = actual_samples - num_train - num_val

    indices = np.random.permutation(actual_samples)
    train_idx = indices[:num_train]
    val_idx = indices[num_train:num_train + num_val]
    test_idx = indices[num_train + num_val:]

    train_data = (
        spectra_array[train_idx],
        labels_array[train_idx],
        wavelengths_array[train_idx[0]] if train_idx.size > 0 else wavelengths_array[0],
        [atom_percentages_array[i] for i in train_idx]
    )
    val_data = (
        spectra_array[val_idx],
        labels_array[val_idx],
        wavelengths_array[val_idx[0]] if val_idx.size > 0 else wavelengths_array[0],
        [atom_percentages_array[i] for i in val_idx]
    )
    test_data = (
        spectra_array[test_idx],
        labels_array[test_idx],
        wavelengths_array[test_idx[0]] if test_idx.size > 0 else wavelengths_array[0],
        [atom_percentages_array[i] for i in test_idx]
    )

    # Simpan ke file HDF5
    temp_output_path = os.path.join(processed_dir, "dataset_500_samples_split1.h5")
    with h5py.File(temp_output_path, "w") as f:
        for group_name, (spectra, labels, wavelengths, atom_percentages) in [
            ("train", train_data),
            ("validation", val_data),
            ("test", test_data)
        ]:
            grp = f.create_group(group_name)
            grp.create_dataset("spectra", data=spectra)
            grp.create_dataset("labels", data=labels)
            grp.create_dataset("wavelengths", data=wavelengths)
            grp.create_dataset("atom_percentages", data=atom_percentages)

    os.makedirs(drive_processed_dir, exist_ok=True)
    drive_output_path = os.path.join(drive_processed_dir, "dataset_500_samples_split1.h5")
    shutil.move(temp_output_path, drive_output_path)
    print(f"Dataset saved to {drive_output_path}")
    print(f"Generated {actual_samples} valid samples out of {num_samples} requested.")

def main():
    """Fungsi utama untuk menjalankan simulasi dan menghasilkan dataset."""
    pd.set_option('future.no_silent_downcasting', True)

    # Mount Google Drive
    mountpoint = '/content/drive'
    try:
        drive.flush_and_unmount()
    except ValueError:
        pass
    os.makedirs(mountpoint, exist_ok=True)
    drive.mount(mountpoint, force_remount=True)

    # Direktori dan file
    drive_base_dir = "/content/drive/MyDrive/libs_lstm"
    data_dir = os.path.join(drive_base_dir, "data/raw/HDF5")
    processed_dir = "/content"
    drive_processed_dir = os.path.join(drive_base_dir, "data/processed")

    nist_source_path = os.path.join(data_dir, "nist_data(1).h5")
    nist_target_path = "/content/nist_data(1).h5"
    atomic_data_source_path = os.path.join(data_dir, "atomic_data1.h5")
    atomic_data_target_path = "/content/atomic_data1.h5"
    json_map_path = os.path.join(drive_base_dir, "data/processed/element_map.json")

    # Salin file ke lokal
    for source, target in [(nist_source_path, nist_target_path), (atomic_data_source_path, atomic_data_target_path)]:
        if not os.path.exists(source):
            raise FileNotFoundError(f"File not found at {source}")
        shutil.copy(source, target)

    # Muat element_map
    if not os.path.exists(json_map_path):
        raise FileNotFoundError(f"element_map.json not found at {json_map_path}")
    with open(json_map_path, 'r') as f:
        element_map = json.load(f)

    # Validasi element_map
    for elem in REQUIRED_ELEMENTS:
        if elem not in element_map or not isinstance(element_map[elem], list) or abs(sum(element_map[elem]) - 1.0) > 1e-6:
            raise ValueError(f"Invalid element_map for {elem}: must be a list summing to 1.0")

    # Muat energi ionisasi
    global ionization_energies
    ionization_energies = {}
    with h5py.File(atomic_data_target_path, 'r') as f:
        dset = f['elements']
        columns = dset.attrs['columns']
        data = [
            [item[0], item[1].decode('utf-8'), item[2].decode('utf-8'), item[3].decode('utf-8'),
             item[4].decode('utf-8'), item[5], item[6].decode('utf-8')]
            for item in dset[:]
        ]
        df_ionization = pd.DataFrame(data, columns=columns)
        for _, row in df_ionization.iterrows():
            ionization_energies[row["Sp. Name"]] = float(row["Ionization Energy (eV)"])

    # Validasi energi ionisasi
    for elem in REQUIRED_ELEMENTS:
        base_elem, ion = elem.split('_')
        ion_level = 'I' if ion == '1' else 'II'
        sp_name = f"{base_elem} {ion_level}"
        if sp_name not in ionization_energies:
            print(f"Warning: No ionization energy for {sp_name}, using 0.0 eV")
            ionization_energies[sp_name] = 0.0

    # Muat data NIST
    fetcher = DataFetcher(nist_target_path)
    nist_data_dict = {}
    delta_E_max_dict = {}
    for elem in REQUIRED_ELEMENTS:
        element, ion = elem.split('_')
        data, delta_E = fetcher.get_nist_data(element, int(ion))
        nist_data_dict[elem] = data
        delta_E_max_dict[elem] = delta_E
        if not data:
            print(f"Warning: No NIST data for {elem}")

    # Buat simulator
    simulators = []
    for elem in REQUIRED_ELEMENTS:
        if nist_data_dict[elem]:
            element, ion = elem.split('_')
            ion_energy = ionization_energies.get(f"{element} {['I', 'II'][int(ion)-1]}", 0.0)
            simulator = SpectrumSimulator(
                nist_data_dict[elem],
                element,
                int(ion),
                SIMULATION_CONFIG["temperature_range"],
                ion_energy,
                SIMULATION_CONFIG
            )
            simulators.append(simulator)

    if not simulators:
        raise ValueError("No valid simulators created. Check NIST data.")

    # Hasilkan dataset
    generate_dataset(
        SIMULATION_CONFIG["num_samples"],
        simulators,
        delta_E_max_dict,
        processed_dir,
        drive_processed_dir
    )

if __name__ == "__main__":
    main()

#Versi Stabil Multi-elemen Stratifikasi Suhu Densitas
import os
import numpy as np
import pandas as pd
import json
import re
import torch
import torch.nn.functional as F
from scipy.signal.windows import gaussian
import h5py
from tqdm import tqdm
import shutil
from typing import List, Dict, Tuple, Optional
from google.colab import drive

# Simulation configuration
SIMULATION_CONFIG = {
    "resolution": 4096,
    "wl_range": (200, 900),
    "sigma": 0.1,  # nm, for Gaussian broadening
    "target_max_intensity": 0.8,
    "convolution_sigma": 0.1,  # nm, for spectrum convolution
    "num_samples": 10000,
    "temperature_range": [6000, 8000, 10000, 12000, 14000, 15000],  # K
    "electron_density_range": np.logspace(15, 17, 10).tolist(),  # cm^-3, from 1e15 to 5e17
}

# Physical constants
PHYSICAL_CONSTANTS = {
    "k_B": 8.617333262145e-5,  # eV/K
    "m_e": 9.1093837e-31,      # kg
    "h": 4.135667696e-15,      # eV·s
}

# Elements and ions
BASE_ELEMENTS = ["Si", "Al", "Fe", "Ca", "O", "Na", "N", "Ni",  "Cr", "Cl"]
REQUIRED_ELEMENTS = [f"{elem}_{ion}" for elem in BASE_ELEMENTS for ion in [1, 2]]

def calculate_lte_electron_density(temp: float, delta_E: float) -> float:
    """Calculate minimum electron density for LTE based on temperature and Delta E.

    Args:
        temp: Temperature in Kelvin.
        delta_E: Transition energy difference in eV.

    Returns:
        Minimum electron density in cm^-3.
    """
    return 1.6e12 * (temp ** 0.5) * (delta_E ** 3)

def get_valid_combinations(temperature_range: List[float], electron_density_range: List[float], delta_E_max_dict: Dict[str, float]) -> List[Tuple[float, float]]:
    """Generate valid temperature and electron density combinations based on LTE constraints.

    Args:
        temperature_range: List of temperatures in Kelvin.
        electron_density_range: List of electron density in cm^-3.
        delta_E_max_dict: Dictionary of maximum Delta E per element and ion.

    Returns:
        List of valid (temperature, electron_density) tuples.
    """
    combinations = []
    for T in temperature_range:
        delta_E_values = [v for v in delta_E_max_dict.values() if v > 0]
        delta_E_max = max(delta_E_values) if delta_E_values else 4.0
        n_e_min = calculate_lte_electron_density(T, delta_E_max)
        valid_n_e = [n_e for n_e in electron_density_range if n_e >= n_e_min]
        for n_e in valid_n_e:
            combinations.append((T, n_e))
    return combinations

class DataFetcher:
    """Fetch spectral data from NIST HDF5 file for specific elements and ions."""

    def __init__(self, hdf_path: str):
        self.hdf_path = hdf_path
        self.delta_E_max = {}  # Cache maximum Delta E per element and ion

    def get_nist_data(self, element: str, sp_num: int) -> Tuple[List[List], float]:
        """Fetch spectral data for an element and ion, and calculate maximum Delta E.

        Args:
            element: Element symbol (e.g., 'Si').
            sp_num: Ionization number (1 for neutral, 2 for +1 ion).

        Returns:
            Tuple of spectral data list (wavelength, Aki, Ek, Ei, g_i, g_k, Acc)
            and maximum Delta E in eV. Returns ([], 0.0) if failed.
        """
        try:
            with pd.HDFStore(self.hdf_path, mode='r') as store:
                df = store.get('nist_spectroscopy_data')
                filtered_df = df[(df['element'] == element) & (df['sp_num'] == sp_num)]
                required_columns = ['ritz_wl_air(nm)', 'Aki(s^-1)', 'Ek(eV)', 'Ei(eV)', 'g_i', 'g_k']

                if filtered_df.empty or not all(col in df.columns for col in required_columns):
                    print(f"No data found for {element}_{sp_num} in NIST dataset")
                    return [], 0.0

                filtered_df = filtered_df.dropna(subset=required_columns)

                # Convert columns to numeric
                filtered_df['ritz_wl_air(nm)'] = pd.to_numeric(filtered_df['ritz_wl_air(nm)'], errors='coerce')
                for col in ['Ek(eV)', 'Ei(eV)', 'Aki(s^-1)', 'g_i', 'g_k']:
                    filtered_df[col] = pd.to_numeric(
                        filtered_df[col].apply(lambda x: re.sub(r'[^\d.-]', '', str(x)) if re.sub(r'[^\d.-]', '', str(x)) else None),
                        errors='coerce'
                    )

                filtered_df = filtered_df.dropna(subset=['ritz_wl_air(nm)', 'Ek(eV)', 'Ei(eV)', 'Aki(s^-1)', 'g_i', 'g_k'])

                # Filter wavelength range
                filtered_df = filtered_df[
                    (filtered_df['ritz_wl_air(nm)'] >= SIMULATION_CONFIG["wl_range"][0]) &
                    (filtered_df['ritz_wl_air(nm)'] <= SIMULATION_CONFIG["wl_range"][1])
                ]

                # Calculate Delta E (eV)
                filtered_df['delta_E'] = abs(filtered_df['Ek(eV)'] - filtered_df['Ei(eV)'])
                if filtered_df.empty:
                    print(f"No valid transitions for {element}_{sp_num} in wavelength range")
                    return [], 0.0

                filtered_df = filtered_df.sort_values(by='Aki(s^-1)', ascending=False)
                delta_E_max = filtered_df['delta_E'].max()
                delta_E_max = 0.0 if pd.isna(delta_E_max) else delta_E_max
                self.delta_E_max[f"{element}_{sp_num}"] = delta_E_max

                return filtered_df[required_columns + ['Acc']].values.tolist(), delta_E_max
        except Exception as e:
            print(f"Error fetching NIST data for {element}_{sp_num}: {str(e)}")
            return [], 0.0

class SpectrumSimulator:
    """Simulate emission spectra for a single element and ion at various temperatures."""

    def __init__(
        self,
        nist_data: List[List],
        element: str,
        ion: int,
        temperatures: List[float],
        ionization_energy: float,
        config: Dict = SIMULATION_CONFIG
    ):
        self.nist_data = nist_data
        self.element = element
        self.ion = ion
        self.temperatures = temperatures
        self.ionization_energy = ionization_energy
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.sigma = config["sigma"]
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.gaussian_cache = {}
        self.element_label = f"{element}_{ion}"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def partition_function(self, energy_levels: List[float], degeneracies: List[float], temperature: float) -> float:
        """Calculate partition function for energy levels at a given temperature."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return sum(g * np.exp(-E / (k_B * temperature)) for g, E in zip(degeneracies, energy_levels) if E is not None) or 1.0

    def calculate_intensity(self, temperature: float, energy: float, degeneracy: float, einstein_coeff: float, Z: float) -> float:
        """Calculate spectral line intensity under LTE conditions."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return (degeneracy * einstein_coeff * np.exp(-energy / (k_B * temperature))) / Z

    def gaussian_profile(self, center: float) -> np.ndarray:
        """Generate Gaussian profile for a center wavelength."""
        if center not in self.gaussian_cache:
            x_tensor = torch.tensor(self.wavelengths, device=self.device, dtype=torch.float32)
            center_tensor = torch.tensor(center, device=self.device, dtype=torch.float32)
            sigma_tensor = torch.tensor(self.sigma, device=self.device, dtype=torch.float32)
            gaussian = torch.exp(-0.5 * ((x_tensor - center_tensor) / sigma_tensor) ** 2) / (sigma_tensor * torch.sqrt(torch.tensor(2 * np.pi)))
            self.gaussian_cache[center] = gaussian.cpu().numpy().astype(np.float32)
        return self.gaussian_cache[center]

    def simulate(self, atom_percentage: float = 1.0) -> Tuple[np.ndarray, List[np.ndarray], List[List[int]], List[List[int]], List[float], List[List[Dict]], List[np.ndarray]]:
        """Simulate spectra for all given temperatures.

        Returns:
            Tuple of wavelengths, spectra, peak indices, peak labels,
            temperatures, intensity data, and element contributions.
        """
        spectra, peak_indices, peak_labels, temperatures, intensity_data, contributions = [], [], [], [], [], []
        levels = {}

        for data in self.nist_data:
            try:
                wl, Aki, Ek, Ei, gi, gk, _ = data
                if all(v is not None for v in [wl, Aki, Ek, Ei, gi, gk]):
                    Ek = float(Ek)
                    Ei = float(Ei)
                    if Ei not in levels:
                        levels[Ei] = float(gi)
                    if Ek not in levels:
                        levels[Ek] = float(gk)
            except (ValueError, TypeError):
                continue

        if not levels:
            print(f"Warning: No valid energy levels for {self.element_label}")
            return self.wavelengths, [], [], [], [], [], []

        energy_levels = list(levels.keys())
        degeneracies = list(levels.values())

        for temp in self.temperatures:
            Z = self.partition_function(energy_levels, degeneracies, temp)
            intensities = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
            element_contributions = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
            peak_idx, peak_label, temp_intensity_data = [], [], []

            for data in self.nist_data:
                try:
                    wl, Aki, Ek, Ei, gi, gk, _ = data
                    if all(v is not None for v in [wl, Aki, Ek, Ei, gi, gk]):
                        wl = float(wl)
                        Aki = float(Aki)
                        Ek = float(Ek)
                        intensity = self.calculate_intensity(temp, Ek, float(gk), Aki, Z)
                        idx = np.searchsorted(self.wavelengths, wl)
                        if 0 <= idx < self.resolution:
                            gaussian_contribution = torch.tensor(
                                intensity * atom_percentage * self.gaussian_profile(wl),
                                device=self.device,
                                dtype=torch.float32
                            )
                            start_idx = max(0, idx - len(gaussian_contribution) // 2)
                            end_idx = min(self.resolution, start_idx + len(gaussian_contribution))
                            if start_idx < end_idx:
                                intensities[start_idx:end_idx] += gaussian_contribution[:end_idx - start_idx]
                                element_contributions[start_idx:end_idx] += gaussian_contribution[:end_idx - start_idx]
                            temp_intensity_data.append({
                                'wavelength': wl,
                                'intensity': intensity * atom_percentage,
                                'element_label': self.element_label,
                                'index': idx
                            })
                            class_idx = REQUIRED_ELEMENTS.index(self.element_label) if self.element_label in REQUIRED_ELEMENTS else len(REQUIRED_ELEMENTS)
                            peak_idx.append(idx)
                            peak_label.append(class_idx)
                except (ValueError, TypeError):
                    continue

            spectra.append(intensities.cpu().numpy())
            peak_indices.append(peak_idx)
            peak_labels.append(peak_label)
            temperatures.append(temp)
            intensity_data.append(temp_intensity_data)
            contributions.append(element_contributions.cpu().numpy())

        return self.wavelengths, spectra, peak_indices, peak_labels, temperatures, intensity_data, contributions

class MixedSpectrumSimulator:
    """Combine spectra from multiple elements based on atomic proportions."""

    def __init__(self, simulators: List[SpectrumSimulator], config: Dict, delta_E_max: Dict[str, float]):
        self.simulators = simulators
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.convolution_sigma = config["convolution_sigma"]
        self.electron_density_range = config["electron_density_range"]
        self.delta_E_max = delta_E_max
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.intensity_threshold = 0.01
        self.current_T = 0.0
        self.current_n_e = 0.0

    def normalize_intensity(self, intensity: np.ndarray, target_max: float) -> np.ndarray:
        """Normalize spectrum intensity to target maximum."""
        intensity_tensor = torch.tensor(intensity, device=self.device, dtype=torch.float32)
        max_intensity = torch.max(torch.abs(intensity_tensor))
        if max_intensity == 0:
            return intensity
        return (intensity_tensor / max_intensity * target_max).cpu().numpy()

    def convolve_spectrum(self, spectrum: np.ndarray, sigma_nm: float) -> np.ndarray:
        """Convolve spectrum with Gaussian kernel."""
        spectrum_tensor = torch.tensor(spectrum, dtype=torch.float32, device=self.device)
        wavelength_step = (self.wavelengths[-1] - self.wavelengths[0]) / (len(self.wavelengths) - 1)
        sigma_points = sigma_nm / wavelength_step
        kernel_size = int(6 * sigma_points) | 1
        kernel = torch.tensor(
            gaussian(kernel_size, sigma_points) / np.sum(gaussian(kernel_size, sigma_points)),
            device=self.device,
            dtype=torch.float32
        )
        kernel = kernel.unsqueeze(0).unsqueeze(0)
        spectrum_tensor = spectrum_tensor.unsqueeze(0).unsqueeze(0)
        convolved = F.conv1d(spectrum_tensor, kernel, padding=kernel_size//2).squeeze().cpu().numpy()
        return convolved.astype(np.float32)

    def saha_ratio(self, ion_energy: float, temp: float, electron_density: float) -> float:
        """Calculate Saha ratio for ionization."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        m_e = PHYSICAL_CONSTANTS["m_e"]
        h = PHYSICAL_CONSTANTS["h"]
        two_pi_me_kT_h2 = (2 * np.pi * m_e * (k_B * temp * 1.60217662e-16) / (h * 1.60217662e-19) ** 2) ** (3/2)
        two_pi_me_kT_h2 /= 1e6
        U_i = 1.0
        U_ip1 = 1.0
        saha_factor = (2 * U_ip1 / U_i) * two_pi_me_kT_h2 / electron_density
        return saha_factor * np.exp(-ion_energy / (k_B * temp))

    def generate_sample(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray], Optional[Dict]]:
        """Generate a single mixed spectrum sample with specified temperature and electron density."""
        temp = self.current_T
        electron_density = self.current_n_e

        # Select elements randomly
        num_target_elements = 7
        selected_base_elements = np.random.choice(BASE_ELEMENTS, num_target_elements, replace=False)
        selected_pairs = [(elem, f"{elem}_1", f"{elem}_2") for elem in selected_base_elements]

        # Calculate maximum Delta E
        delta_E_values = []
        for base_elem, elem_neutral, elem_ion in selected_pairs:
            for elem in [elem_neutral, elem_ion]:
                delta_E = self.delta_E_max.get(elem, 0.0)
                if delta_E > 0.0:
                    delta_E_values.append(delta_E)
        delta_E_max = max(delta_E_values) if delta_E_values else 4.0

        # Warn about self-absorption
        if electron_density > 5e16 and temp < 8000:
            print(f"Warning: High n_e ({electron_density:.2e} cm^-3) and low T ({temp} K) may cause self-absorption.")

        # Calculate atomic fractions using Saha ratio
        atom_percentages_dict = {}
        total_target_percentage = 0.0
        for base_elem, elem_neutral, elem_ion in selected_pairs:
            ion_energy = ionization_energies.get(f"{base_elem} I", 0.0)
            if ion_energy == 0.0:
                print(f"Warning: No ionization energy for {base_elem} I")
                continue
            saha_ratio = self.saha_ratio(ion_energy, temp, electron_density)
            total_percentage = np.random.uniform(5, 20)
            fraction_neutral = 1 / (1 + saha_ratio)
            fraction_ion = saha_ratio / (1 + saha_ratio)
            percentage_neutral = total_percentage * fraction_neutral
            percentage_ion = total_percentage * fraction_ion
            atom_percentages_dict[elem_neutral] = percentage_neutral / 100.0
            atom_percentages_dict[elem_ion] = percentage_ion / 100.0
            total_target_percentage += total_percentage

        # Scale percentages to sum to 100%
        if total_target_percentage != 0:
            scaling_factor = 100.0 / total_target_percentage
            for key in atom_percentages_dict:
                atom_percentages_dict[key] *= scaling_factor
        else:
            return None, None, None, None

        atom_percentages_dict['temperature'] = float(temp)
        atom_percentages_dict['electron_density'] = float(electron_density)
        atom_percentages_dict['delta_E_max'] = float(delta_E_max)

        selected_target_elements = [k for k in atom_percentages_dict.keys() if k not in ['temperature', 'electron_density', 'delta_E_max']]
        atom_percentages = np.array([atom_percentages_dict[elem] for elem in selected_target_elements], dtype=np.float32)
        selected_simulators = [sim for sim in self.simulators if f"{sim.element}_{sim.ion}" in selected_target_elements]

        if not selected_simulators:
            print(f"Warning: No valid simulators for {selected_target_elements}")
            return None, None, None, None

        mixed_spectrum = np.zeros(self.resolution, dtype=np.float32)
        element_contributions = np.zeros((len(selected_simulators), self.resolution), dtype=np.float32)

        for sim_idx, simulator in enumerate(selected_simulators):
            idx = selected_target_elements.index(f"{simulator.element}_{simulator.ion}")
            atom_percentage = atom_percentages[idx]
            wavelengths, element_spectra, _, _, temps, _, contributions = simulator.simulate(atom_percentage)
            for spectrum, t, contrib in zip(element_spectra, temps, contributions):
                if t == temp:
                    mixed_spectrum += spectrum
                    element_contributions[sim_idx] = contrib
                    break

        if np.max(mixed_spectrum) == 0:
            print(f"Warning: No spectrum generated for T={temp} K")
            return None, None, None, None

        convolved_spectrum = self.convolve_spectrum(mixed_spectrum, self.convolution_sigma)
        normalized_spectrum = self.normalize_intensity(convolved_spectrum, SIMULATION_CONFIG["target_max_intensity"])

        labels = np.zeros(self.resolution, dtype=np.int32)
        contributions_array = np.zeros((self.resolution, len(REQUIRED_ELEMENTS)), dtype=np.float32)

        for idx in range(self.resolution):
            contributions_at_point = element_contributions[:, idx]
            total_intensity = np.sum(contributions_at_point)
            if total_intensity >= self.intensity_threshold:
                dominant_sim_idx = np.argmax(contributions_at_point)
                dominant_element = selected_simulators[dominant_sim_idx].element_label
                if dominant_element in REQUIRED_ELEMENTS:
                    dominant_label = REQUIRED_ELEMENTS.index(dominant_element)
                    labels[idx] = dominant_label + 1
                    contributions_array[idx, dominant_label] = normalized_spectrum[idx]

        atom_percentages_dict = {
            k: float(v * 100) if k not in ['temperature', 'electron_density', 'delta_E_max'] else float(v)
            for k, v in atom_percentages_dict.items()
        }
        return self.wavelengths, normalized_spectrum, labels, atom_percentages_dict

def generate_dataset(
    num_samples: int,
    simulators: List[SpectrumSimulator],
    delta_E_max: Dict[str, float],
    processed_dir: str,
    drive_processed_dir: str
) -> None:
    """Generate and save dataset to HDF5 with uniform temperature distribution across iterations."""
    mixed_simulator = MixedSpectrumSimulator(simulators, SIMULATION_CONFIG, delta_E_max)
    spectra_list, labels_list, wavelengths_list, atom_percentages_list = [], [], [], []

    # Get temperature range and electron density range
    temperature_range = SIMULATION_CONFIG["temperature_range"]
    electron_density_range = SIMULATION_CONFIG["electron_density_range"]
    num_temperatures = len(temperature_range)

    # Calculate base samples per temperature and distribute remainder
    samples_per_temp = num_samples // num_temperatures
    remainder = num_samples % num_temperatures

    # Create separate lists of (T, n_e) pairs for each temperature
    temp_specific_params = {T: [] for T in temperature_range}
    for i, T in enumerate(temperature_range):
        n_samples_for_temp = samples_per_temp + (1 if i < remainder else 0)
        print(f"Processing T={T} K with {n_samples_for_temp} samples")

        # Calculate minimum electron density for LTE
        delta_E_values = [v for v in delta_E_max.values() if v > 0]
        delta_E_max_value = max(delta_E_values) if delta_E_values else 4.0
        n_e_min = calculate_lte_electron_density(T, delta_E_max_value)
        print(f"n_e_min for T={T} K: {n_e_min:.2e} cm^-3")

        # Select valid electron densities for this temperature
        valid_n_e = [n_e for n_e in electron_density_range if n_e >= n_e_min]
        if not valid_n_e:
            print(f"Warning: No valid electron densities for T={T} K (n_e_min={n_e_min:.2e} cm^-3). Using available range.")
            valid_n_e = electron_density_range  # Fallback to full range

        # Limit the number of Ne values to use, ensuring fairness
        max_ne_per_temp = min(len(valid_n_e), 5)  # Use up to 5 Ne values per temperature
        if max_ne_per_temp == 0:
            max_ne_per_temp = len(electron_density_range)  # Fallback to all if none valid
        selected_n_e = np.random.choice(valid_n_e, size=max_ne_per_temp, replace=False).tolist()

        # Repeat selected Ne randomly to meet sample count
        if len(selected_n_e) < n_samples_for_temp:
            additional_n_e = np.random.choice(selected_n_e, size=n_samples_for_temp - len(selected_n_e), replace=True)
            selected_n_e.extend(additional_n_e)

        # Add (T, n_e) pairs to the temperature-specific list
        temp_specific_params[T].extend([(T, n_e) for n_e in selected_n_e[:n_samples_for_temp]])

    # Verify initial distribution
    sample_counts = {T: len(params) for T, params in temp_specific_params.items()}
    for T, count in sample_counts.items():
        print(f"Initial distribution - Temperature {T} K: {count} samples ({count/num_samples*100:.2f}%)")

    # Create final sample_params by round-robin selection to ensure even distribution
    sample_params = []
    indices = {T: 0 for T in temperature_range}  # Track current index for each temperature
    remaining = {T: len(params) for T, params in temp_specific_params.items()}
    total_remaining = sum(remaining.values())

    while total_remaining > 0:
        for T in temperature_range:
            if remaining[T] > 0:
                idx = indices[T]
                sample_params.append(temp_specific_params[T][idx])
                indices[T] += 1
                remaining[T] -= 1
                total_remaining -= 1

    # Ensure we have exactly num_samples
    if len(sample_params) < num_samples:
        print(f"Warning: Only {len(sample_params)} samples generated, less than requested {num_samples}. Duplicating samples...")
        while len(sample_params) < num_samples:
            idx = np.random.randint(0, len(sample_params))
            sample_params.append(sample_params[idx])
    elif len(sample_params) > num_samples:
        sample_params = sample_params[:num_samples]

    # Debug distribution after round-robin
    from collections import Counter
    temps = [param[0] for param in sample_params]
    temp_counts = Counter(temps)
    for temp, count in temp_counts.items():
        print(f"Final distribution - Temperature {temp} K: {count} samples ({count/num_samples*100:.2f}%)")

    # Generate samples
    for i, (T, n_e) in enumerate(tqdm(sample_params, desc="Generating dataset")):
        mixed_simulator.current_T = T
        mixed_simulator.current_n_e = n_e
        print(f"Iteration {i + 1}: Processing T={T} K, n_e={n_e:.2e} cm^-3")  # Add this line to show T and n_e
        result = mixed_simulator.generate_sample()
        if result[0] is not None:
            wavelengths, spectrum, labels, atom_percentages = result
            spectra_list.append(spectrum)
            labels_list.append(labels)
            wavelengths_list.append(wavelengths)
            atom_percentages_list.append(atom_percentages)

        # Debug distribution every 10% progress
        if (i + 1) % (num_samples // 10) == 0:
            current_temps = [param[0] for param in sample_params[:i + 1]]
            current_counts = Counter(current_temps)
            print(f"Progress {(i + 1)/num_samples*100:.1f}%:")
            for temp, count in current_counts.items():
                print(f"Temperature {temp} K: {count} samples ({count/(i + 1)*100:.2f}%)")

    if not spectra_list:
        raise ValueError("No valid samples generated. Check NIST data or simulator configuration.")

    spectra_array = np.array(spectra_list, dtype=np.float32)
    labels_array = np.array(labels_list, dtype=np.int32)
    wavelengths_array = np.array(wavelengths_list, dtype=np.float32)
    atom_percentages_array = [json.dumps(d).encode('utf-8') for d in atom_percentages_list]

    # Split data into train, validation, and test
    actual_samples = len(spectra_list)
    num_train = int(0.7 * actual_samples)
    num_val = int(0.15 * actual_samples)
    num_test = actual_samples - num_train - num_val

    indices = np.random.permutation(actual_samples)
    train_idx = indices[:num_train]
    val_idx = indices[num_train:num_train + num_val]
    test_idx = indices[num_train + num_val:]

    train_data = (
        spectra_array[train_idx],
        labels_array[train_idx],
        wavelengths_array[train_idx[0]] if train_idx.size > 0 else wavelengths_array[0],
        [atom_percentages_array[i] for i in train_idx]
    )
    val_data = (
        spectra_array[val_idx],
        labels_array[val_idx],
        wavelengths_array[val_idx[0]] if val_idx.size > 0 else wavelengths_array[0],
        [atom_percentages_array[i] for i in val_idx]
    )
    test_data = (
        spectra_array[test_idx],
        labels_array[test_idx],
        wavelengths_array[test_idx[0]] if test_idx.size > 0 else wavelengths_array[0],
        [atom_percentages_array[i] for i in test_idx]
    )

    # Save to HDF5 with dynamic filename
    output_filename = f"{num_samples}-dataset.h5"
    temp_output_path = os.path.join(processed_dir, output_filename)
    with h5py.File(temp_output_path, 'w') as f:
        for group_name, (spectra, labels, wavelengths, atom_percentages) in [
            ("train", train_data),
            ("validation", val_data),
            ("test", test_data)
        ]:
            grp = f.create_group(group_name)
            grp.create_dataset("spectra", data=spectra)
            grp.create_dataset("labels", data=labels)
            grp.create_dataset("wavelengths", data=wavelengths)
            grp.create_dataset("atom_percentages", data=atom_percentages)

    os.makedirs(drive_processed_dir, exist_ok=True)
    drive_output_path = os.path.join(drive_processed_dir, output_filename)
    shutil.move(temp_output_path, drive_output_path)
    print(f"Dataset saved to {drive_output_path}")
    print(f"Generated {actual_samples} valid samples out of {num_samples} requested.")
def main():
    """Main function to run simulation and generate dataset."""
    pd.set_option('future.no_silent_downcasting', True)

    # Mount Google Drive
    mountpoint = '/content/drive'
    try:
        drive.flush_and_unmount()
    except ValueError:
        pass
    os.makedirs(mountpoint, exist_ok=True)
    drive.mount(mountpoint, force_remount=True)

    # Directories and files
    drive_base_dir = "/content/drive/MyDrive/libs_lstm"
    data_dir = os.path.join(drive_base_dir, "data/raw/HDF5")
    processed_dir = "/content"
    drive_processed_dir = os.path.join(drive_base_dir, "data/processed")

    nist_source_path = os.path.join(data_dir, "nist_data(1).h5")
    nist_target_path = "/content/nist_data(1).h5"
    atomic_data_source_path = os.path.join(data_dir, "atomic_data1.h5")
    atomic_data_target_path = "/content/atomic_data1.h5"
    json_map_path = os.path.join(drive_base_dir, "data/processed", "element_map.json")

    # Copy files to local
    for source, target in tqdm([(nist_source_path, nist_target_path), (atomic_data_source_path, atomic_data_target_path)], desc="Copying files"):
        if not os.path.exists(source):
            raise FileNotFoundError(f"File not found at {source}")
        shutil.copy(source, target)

    # Load element_map
    if not os.path.exists(json_map_path):
        raise FileNotFoundError(f"element_map.json not found at {json_map_path}")
    with open(json_map_path, 'r') as f:
        element_map = json.load(f)

    # Validate element_map
    for elem in REQUIRED_ELEMENTS:
        if elem not in element_map or not isinstance(element_map[elem], list) or abs(sum(element_map[elem]) - 1.0) > 1e-6:
            raise ValueError(f"Invalid element_map for {elem}: must be a list summing to 1.0")

    # Load ionization energies
    global ionization_energies
    ionization_energies = {}
    try:
        with h5py.File(atomic_data_target_path, 'r') as f:
            dset = f['elements']
            # Check if 'columns' attribute exists, otherwise define manually
            if 'columns' in dset.attrs:
                columns = dset.attrs['columns']
            else:
                print("Warning: 'columns' attribute missing in atomic_data1.h5. Using default column names.")
                columns = ['At. num', 'Sp. Name', 'Ion Charge', 'El. Name', 'Prefix', 'Ionization Energy (eV)', 'Suffix']
                print(f"Assumed columns: {columns}")

            # Load data
            data = [
                [item[0], item[1].decode('utf-8'), item[2].decode('utf-8'), item[3].decode('utf-8'),
                 item[4].decode('utf-8'), item[5], item[6].decode('utf-8')]
                for item in dset[:]
            ]
            df_ionization = pd.DataFrame(data, columns=columns)

            # Find the correct column for species and ionization energy
            species_col = None
            ion_energy_col = None
            for col in columns:
                if col.lower() in ['sp.', 'species', 'sp', 'element', 'sp. name']:
                    species_col = col
                if 'ionization' in col.lower() and 'ev' in col.lower():
                    ion_energy_col = col

            if not species_col or not ion_energy_col:
                print(f"Error: Could not find species or ionization energy columns. Available columns: {list(df_ionization.columns)}")
                raise KeyError("Required columns for species or ionization energy not found in atomic_data1.h5")

            # Load ionization energies
            for _, row in df_ionization.iterrows():
                try:
                    ionization_energies[row[species_col]] = float(row[ion_energy_col])
                except (ValueError, TypeError):
                    print(f"Warning: Invalid ionization energy for {row[species_col]}, using 0.0 eV")
                    ionization_energies[row[species_col]] = 0.0

    except KeyError as e:
        print(f"Error loading atomic_data1.h5: {str(e)}. Available columns: {list(df_ionization.columns) if 'df_ionization' in locals() else 'Unknown'}")
        raise
    except Exception as e:
        print(f"Error loading atomic_data1.h5: {str(e)}")
        raise

    # Validate ionization energies
    for elem in REQUIRED_ELEMENTS:
        base_elem, ion = elem.split('_')
        ion_level = 'I' if ion == '1' else 'II'
        sp_name = f"{base_elem} {ion_level}"
        if sp_name not in ionization_energies:
            print(f"Warning: No ionization energy for {sp_name}, using 0.0 eV")
            ionization_energies[sp_name] = 0.0

    # Load NIST data
    fetcher = DataFetcher(nist_target_path)
    nist_data_dict = {}
    delta_E_max_dict = {}
    for elem in REQUIRED_ELEMENTS:
        element, ion = elem.split('_')
        data, delta_E = fetcher.get_nist_data(element, int(ion))
        nist_data_dict[elem] = data
        delta_E_max_dict[elem] = delta_E
        if not data:
            print(f"No NIST data for {elem}")

    # Create simulators
    simulators = []
    for elem in nist_data_dict:
        if nist_data_dict[elem]:
            element, ion = elem.split('_')
            ion_energy = ionization_energies.get(f"{element} {'I' if int(ion) == 1 else 'II'}", 0.0)
            simulator = SpectrumSimulator(
                nist_data_dict[elem],
                element,
                int(ion),
                SIMULATION_CONFIG["temperature_range"],
                ion_energy,
                SIMULATION_CONFIG
            )
            simulators.append(simulator)

    if not simulators:
        raise ValueError("No valid simulators created. Check NIST data.")

    # Generate dataset
    generate_dataset(
        SIMULATION_CONFIG["num_samples"],
        simulators,
        delta_E_max_dict,
        processed_dir,
        drive_processed_dir
    )

if __name__ == "__main__":
    main()

#Dev
import os
import numpy as np
import pandas as pd
import json
import re
import torch
import torch.nn.functional as F
from scipy.signal.windows import gaussian
import h5py
from tqdm import tqdm
import shutil
from typing import List, Dict, Tuple, Optional
from google.colab import drive
from collections import Counter
import hashlib
from datetime import datetime

# Konfigurasi simulasi
SIMULATION_CONFIG = {
    "resolution": 4096,
    "wl_range": (200, 900),
    "sigma": 0.1,  # nm, untuk pelebaran Gaussian
    "target_max_intensity": 0.8,
    "convolution_sigma": 0.1,  # nm, untuk konvolusi spektrum
    "num_samples": 1000,
    "temperature_range": [6000, 8000, 10000, 12000, 14000, 15000],  # K
    "electron_density_range": np.logspace(15, 17, 10).tolist(),  # cm^-3
}

# Konstanta fisika
PHYSICAL_CONSTANTS = {
    "k_B": 8.617333262145e-5,  # eV/K
    "m_e": 9.1093837e-31,      # kg
    "h": 4.135667696e-15,      # eV·s
}

# Elemen dan ion
BASE_ELEMENTS = ["Si", "Al", "Fe", "Ca", "O", "Na", "N", "Ni", "Cr", "Cl"]
REQUIRED_ELEMENTS = [f"{elem}_{ion}" for elem in BASE_ELEMENTS for ion in [1, 2]]

class DataFetcher:
    """Mengambil data spektral dari file HDF5 NIST untuk elemen dan ion tertentu."""

    def __init__(self, hdf_path: str):
        self.hdf_path = hdf_path
        self.delta_E_max: Dict[str, float] = {}

    def get_nist_data(self, element: str, sp_num: int) -> Tuple[List[List], float]:
        """Mengambil data spektral untuk elemen dan ion, serta menghitung Delta E maksimum.

        Args:
            element: Simbol elemen (misalnya, 'Si').
            sp_num: Nomor ionisasi (1 untuk netral, 2 untuk ion +1).

        Returns:
            Tuple berisi daftar data spektral (wavelength, Aki, Ek, Ei, g_i, g_k, Acc)
            dan Delta E maksimum dalam eV. Mengembalikan ([], 0.0) jika gagal.
        """
        try:
            with pd.HDFStore(self.hdf_path, mode='r') as store:
                df = store.get('nist_spectroscopy_data')
                filtered_df = df[(df['element'] == element) & (df['sp_num'] == sp_num)]
                required_columns = ['ritz_wl_air(nm)', 'Aki(s^-1)', 'Ek(eV)', 'Ei(eV)', 'g_i', 'g_k']

                if filtered_df.empty or not all(col in df.columns for col in required_columns):
                    print(f"Tidak ada data untuk {element}_{sp_num} di dataset NIST")
                    return [], 0.0

                filtered_df = filtered_df.dropna(subset=required_columns)

                # Konversi kolom ke numerik
                filtered_df['ritz_wl_air(nm)'] = pd.to_numeric(filtered_df['ritz_wl_air(nm)'], errors='coerce')
                for col in ['Ek(eV)', 'Ei(eV)', 'Aki(s^-1)', 'g_i', 'g_k']:
                    filtered_df[col] = pd.to_numeric(
                        filtered_df[col].apply(lambda x: re.sub(r'[^\d.-]', '', str(x)) if re.sub(r'[^\d.-]', '', str(x)) else None),
                        errors='coerce'
                    )

                filtered_df = filtered_df.dropna(subset=['ritz_wl_air(nm)', 'Ek(eV)', 'Ei(eV)', 'Aki(s^-1)', 'g_i', 'g_k'])

                # Filter rentang panjang gelombang
                filtered_df = filtered_df[
                    (filtered_df['ritz_wl_air(nm)'] >= SIMULATION_CONFIG["wl_range"][0]) &
                    (filtered_df['ritz_wl_air(nm)'] <= SIMULATION_CONFIG["wl_range"][1])
                ]

                # Hitung Delta E (eV)
                filtered_df['delta_E'] = abs(filtered_df['Ek(eV)'] - filtered_df['Ei(eV)'])
                if filtered_df.empty:
                    print(f"Tidak ada transisi valid untuk {element}_{sp_num} di rentang panjang gelombang")
                    return [], 0.0

                filtered_df = filtered_df.sort_values(by='Aki(s^-1)', ascending=False)
                delta_E_max = filtered_df['delta_E'].max()
                delta_E_max = 0.0 if pd.isna(delta_E_max) else delta_E_max
                self.delta_E_max[f"{element}_{sp_num}"] = delta_E_max

                return filtered_df[required_columns + ['Acc']].values.tolist(), delta_E_max
        except Exception as e:
            print(f"Error mengambil data NIST untuk {element}_{sp_num}: {str(e)}")
            return [], 0.0

class SpectrumSimulator:
    """Mensimulasikan spektrum emisi untuk satu elemen dan ion pada berbagai suhu."""

    def __init__(
        self,
        nist_data: List[List],
        element: str,
        ion: int,
        temperatures: List[float],
        ionization_energy: float,
        config: Dict = SIMULATION_CONFIG
    ):
        self.nist_data = nist_data
        self.element = element
        self.ion = ion
        self.temperatures = temperatures
        self.ionization_energy = ionization_energy
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.sigma = config["sigma"]
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.gaussian_cache: Dict[float, np.ndarray] = {}
        self.element_label = f"{element}_{ion}"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def _partition_function(self, energy_levels: List[float], degeneracies: List[float], temperature: float) -> float:
        """Menghitung fungsi partisi untuk tingkat energi pada suhu tertentu."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return sum(g * np.exp(-E / (k_B * temperature)) for g, E in zip(degeneracies, energy_levels) if E is not None) or 1.0

    def _calculate_intensity(self, temperature: float, energy: float, degeneracy: float, einstein_coeff: float, Z: float) -> float:
        """Menghitung intensitas garis spektral dalam kondisi LTE."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return (degeneracy * einstein_coeff * np.exp(-energy / (k_B * temperature))) / Z

    def _gaussian_profile(self, center: float) -> np.ndarray:
        """Menghasilkan profil Gaussian untuk panjang gelombang pusat."""
        if center not in self.gaussian_cache:
            x_tensor = torch.tensor(self.wavelengths, device=self.device, dtype=torch.float32)
            center_tensor = torch.tensor(center, device=self.device, dtype=torch.float32)
            sigma_tensor = torch.tensor(self.sigma, device=self.device, dtype=torch.float32)
            gaussian = torch.exp(-0.5 * ((x_tensor - center_tensor) / sigma_tensor) ** 2) / (sigma_tensor * torch.sqrt(torch.tensor(2 * np.pi)))
            self.gaussian_cache[center] = gaussian.cpu().numpy().astype(np.float32)
        return self.gaussian_cache[center]

    def simulate(self, atom_percentage: float = 1.0) -> Tuple[np.ndarray, List[np.ndarray], List[List[int]], List[List[int]], List[float], List[List[Dict]], List[np.ndarray]]:
        """Mensimulasikan spektrum untuk semua suhu yang diberikan.

        Returns:
            Tuple berisi panjang gelombang, spektrum, indeks puncak, label puncak,
            suhu, data intensitas, dan kontribusi elemen.
        """
        spectra, peak_indices, peak_labels, temperatures, intensity_data, contributions = [], [], [], [], [], []
        levels = {}

        for data in self.nist_data:
            try:
                wl, Aki, Ek, Ei, gi, gk, _ = data
                if all(v is not None for v in [wl, Aki, Ek, Ei, gi, gk]):
                    Ek = float(Ek)
                    Ei = float(Ei)
                    if Ei not in levels:
                        levels[Ei] = float(gi)
                    if Ek not in levels:
                        levels[Ek] = float(gk)
            except (ValueError, TypeError):
                continue

        if not levels:
            print(f"Peringatan: Tidak ada tingkat energi valid untuk {self.element_label}")
            return self.wavelengths, [], [], [], [], [], []

        energy_levels = list(levels.keys())
        degeneracies = list(levels.values())

        for temp in self.temperatures:
            Z = self._partition_function(energy_levels, degeneracies, temp)
            intensities = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
            element_contributions = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
            peak_idx, peak_label, temp_intensity_data = [], [], []

            for data in self.nist_data:
                try:
                    wl, Aki, Ek, Ei, gi, gk, _ = data
                    if all(v is not None for v in [wl, Aki, Ek, Ei, gi, gk]):
                        wl = float(wl)
                        Aki = float(Aki)
                        Ek = float(Ek)
                        intensity = self._calculate_intensity(temp, Ek, float(gk), Aki, Z)
                        idx = np.searchsorted(self.wavelengths, wl)
                        if 0 <= idx < self.resolution:
                            gaussian_contribution = torch.tensor(
                                intensity * atom_percentage * self._gaussian_profile(wl),
                                device=self.device,
                                dtype=torch.float32
                            )
                            start_idx = max(0, idx - len(gaussian_contribution) // 2)
                            end_idx = min(self.resolution, start_idx + len(gaussian_contribution))
                            if start_idx < end_idx:
                                intensities[start_idx:end_idx] += gaussian_contribution[:end_idx - start_idx]
                                element_contributions[start_idx:end_idx] += gaussian_contribution[:end_idx - start_idx]
                            temp_intensity_data.append({
                                'wavelength': wl,
                                'intensity': intensity * atom_percentage,
                                'element_label': self.element_label,
                                'index': idx
                            })
                            class_idx = REQUIRED_ELEMENTS.index(self.element_label) if self.element_label in REQUIRED_ELEMENTS else len(REQUIRED_ELEMENTS)
                            peak_idx.append(idx)
                            peak_label.append(class_idx)
                except (ValueError, TypeError):
                    continue

            spectra.append(intensities.cpu().numpy())
            peak_indices.append(peak_idx)
            peak_labels.append(peak_label)
            temperatures.append(temp)
            intensity_data.append(temp_intensity_data)
            contributions.append(element_contributions.cpu().numpy())

        return self.wavelengths, spectra, peak_indices, peak_labels, temperatures, intensity_data, contributions

class MixedSpectrumSimulator:
    """Menggabungkan spektrum dari beberapa elemen berdasarkan proporsi atom."""

    def __init__(self, simulators: List[SpectrumSimulator], config: Dict, delta_E_max: Dict[str, float]):
        self.simulators = simulators
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.convolution_sigma = config["convolution_sigma"]
        self.electron_density_range = config["electron_density_range"]
        self.delta_E_max = delta_E_max
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.intensity_threshold = 0.01
        self.current_T: float = 0.0
        self.current_n_e: float = 0.0

    def _normalize_intensity(self, intensity: np.ndarray, target_max: float) -> np.ndarray:
        """Menormalkan intensitas spektrum ke maksimum target."""
        intensity_tensor = torch.tensor(intensity, device=self.device, dtype=torch.float32)
        max_intensity = torch.max(torch.abs(intensity_tensor))
        if max_intensity == 0:
            return intensity
        return (intensity_tensor / max_intensity * target_max).cpu().numpy()

    def _convolve_spectrum(self, spectrum: np.ndarray, sigma_nm: float) -> np.ndarray:
        """Mengonvolusi spektrum dengan kernel Gaussian."""
        spectrum_tensor = torch.tensor(spectrum, dtype=torch.float32, device=self.device)
        wavelength_step = (self.wavelengths[-1] - self.wavelengths[0]) / (len(self.wavelengths) - 1)
        sigma_points = sigma_nm / wavelength_step
        kernel_size = int(6 * sigma_points) | 1
        kernel = torch.tensor(
            gaussian(kernel_size, sigma_points) / np.sum(gaussian(kernel_size, sigma_points)),
            device=self.device,
            dtype=torch.float32
        )
        kernel = kernel.unsqueeze(0).unsqueeze(0)
        spectrum_tensor = spectrum_tensor.unsqueeze(0).unsqueeze(0)
        convolved = F.conv1d(spectrum_tensor, kernel, padding=kernel_size//2).squeeze().cpu().numpy()
        return convolved.astype(np.float32)

    def _saha_ratio(self, ion_energy: float, temp: float, electron_density: float) -> float:
        """Menghitung rasio Saha untuk ionisasi."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        m_e = PHYSICAL_CONSTANTS["m_e"]
        h = PHYSICAL_CONSTANTS["h"]
        two_pi_me_kT_h2 = (2 * np.pi * m_e * (k_B * temp * 1.60217662e-16) / (h * 1.60217662e-19) ** 2) ** (3/2)
        two_pi_me_kT_h2 /= 1e6
        U_i = 1.0
        U_ip1 = 1.0
        saha_factor = (2 * U_ip1 / U_i) * two_pi_me_kT_h2 / electron_density
        return saha_factor * np.exp(-ion_energy / (k_B * temp))

    def generate_sample(self, ionization_energies: Dict[str, float]) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray], Optional[Dict]]:
        """Menghasilkan sampel spektrum campuran dengan suhu dan densitas elektron tertentu."""
        temp = self.current_T
        electron_density = self.current_n_e

        # Pilih elemen secara acak
        num_target_elements = 7
        selected_base_elements = np.random.choice(BASE_ELEMENTS, num_target_elements, replace=False)
        selected_pairs = [(elem, f"{elem}_1", f"{elem}_2") for elem in selected_base_elements]

        # Hitung Delta E maksimum
        delta_E_values = []
        for base_elem, elem_neutral, elem_ion in selected_pairs:
            for elem in [elem_neutral, elem_ion]:
                delta_E = self.delta_E_max.get(elem, 0.0)
                if delta_E > 0.0:
                    delta_E_values.append(delta_E)
        delta_E_max = max(delta_E_values) if delta_E_values else 4.0

        # Peringatkan tentang self-absorption
        if electron_density > 5e16 and temp < 8000:
            print(f"Peringatan: n_e tinggi ({electron_density:.2e} cm^-3) dan T rendah ({temp} K) dapat menyebabkan self-absorption.")

        # Hitung fraksi atom menggunakan rasio Saha
        atom_percentages_dict = {}
        total_target_percentage = 0.0
        for base_elem, elem_neutral, elem_ion in selected_pairs:
            ion_energy = ionization_energies.get(f"{base_elem} I", 0.0)
            if ion_energy == 0.0:
                print(f"Peringatan: Tidak ada energi ionisasi untuk {base_elem} I")
                continue
            saha_ratio = self._saha_ratio(ion_energy, temp, electron_density)
            total_percentage = np.random.uniform(5, 20)
            fraction_neutral = 1 / (1 + saha_ratio)
            fraction_ion = saha_ratio / (1 + saha_ratio)
            percentage_neutral = total_percentage * fraction_neutral
            percentage_ion = total_percentage * fraction_ion
            atom_percentages_dict[elem_neutral] = percentage_neutral / 100.0
            atom_percentages_dict[elem_ion] = percentage_ion / 100.0
            total_target_percentage += total_percentage

        # Skalakan persentase agar totalnya 100%
        if total_target_percentage != 0:
            scaling_factor = 100.0 / total_target_percentage
            for key in atom_percentages_dict:
                atom_percentages_dict[key] *= scaling_factor
        else:
            return None, None, None, None

        atom_percentages_dict['temperature'] = float(temp)
        atom_percentages_dict['electron_density'] = float(electron_density)
        atom_percentages_dict['delta_E_max'] = float(delta_E_max)

        selected_target_elements = [k for k in atom_percentages_dict.keys() if k not in ['temperature', 'electron_density', 'delta_E_max']]
        atom_percentages = np.array([atom_percentages_dict[elem] for elem in selected_target_elements], dtype=np.float32)
        selected_simulators = [sim for sim in self.simulators if f"{sim.element}_{sim.ion}" in selected_target_elements]

        if not selected_simulators:
            print(f"Peringatan: Tidak ada simulator valid untuk {selected_target_elements}")
            return None, None, None, None

        mixed_spectrum = np.zeros(self.resolution, dtype=np.float32)
        element_contributions = np.zeros((len(selected_simulators), self.resolution), dtype=np.float32)

        for sim_idx, simulator in enumerate(selected_simulators):
            idx = selected_target_elements.index(f"{simulator.element}_{simulator.ion}")
            atom_percentage = atom_percentages[idx]
            wavelengths, element_spectra, _, _, temps, _, contributions = simulator.simulate(atom_percentage)
            for spectrum, t, contrib in zip(element_spectra, temps, contributions):
                if t == temp:
                    mixed_spectrum += spectrum
                    element_contributions[sim_idx] = contrib
                    break

        if np.max(mixed_spectrum) == 0:
            print(f"Peringatan: Tidak ada spektrum dihasilkan untuk T={temp} K")
            return None, None, None, None

        convolved_spectrum = self._convolve_spectrum(mixed_spectrum, self.convolution_sigma)
        normalized_spectrum = self._normalize_intensity(convolved_spectrum, SIMULATION_CONFIG["target_max_intensity"])

        labels = np.zeros(self.resolution, dtype=np.int32)
        contributions_array = np.zeros((self.resolution, len(REQUIRED_ELEMENTS)), dtype=np.float32)

        for idx in range(self.resolution):
            contributions_at_point = element_contributions[:, idx]
            total_intensity = np.sum(contributions_at_point)
            if total_intensity >= self.intensity_threshold:
                dominant_sim_idx = np.argmax(contributions_at_point)
                dominant_element = selected_simulators[dominant_sim_idx].element_label
                if dominant_element in REQUIRED_ELEMENTS:
                    dominant_label = REQUIRED_ELEMENTS.index(dominant_element)
                    labels[idx] = dominant_label + 1
                    contributions_array[idx, dominant_label] = normalized_spectrum[idx]

        atom_percentages_dict = {
            k: float(v * 100) if k not in ['temperature', 'electron_density', 'delta_E_max'] else float(v)
            for k, v in atom_percentages_dict.items()
        }
        return self.wavelengths, normalized_spectrum, labels, atom_percentages_dict

class DatasetGenerator:
    """Menghasilkan dan menyimpan dataset spektral dengan distribusi suhu yang seragam."""

    def __init__(self, config: Dict = SIMULATION_CONFIG):
        self.config = config
        self.temperature_range = config["temperature_range"]
        self.electron_density_range = config["electron_density_range"]
        self.num_samples = config["num_samples"]
        self.combinations_json_path = None
        self.used_combinations = set()

    def _calculate_lte_electron_density(self, temp: float, delta_E: float) -> float:
        """Menghitung densitas elektron minimum untuk LTE berdasarkan suhu dan Delta E."""
        return 1.6e12 * (temp ** 0.5) * (delta_E ** 3)

    def _hash_combination(self, temp: float, n_e: float, atom_percentages: Dict) -> str:
        """Membuat hash unik untuk kombinasi suhu, densitas elektron, dan persentase atom."""
        elements_sorted = sorted(
            [(k, round(v, 6)) for k, v in atom_percentages.items()
             if k not in ['temperature', 'electron_density', 'delta_E_max']],
            key=lambda x: x[0]
        )
        combination_str = f"{temp:.2f}_{n_e:.2e}_{str(elements_sorted)}"
        return hashlib.sha256(combination_str.encode()).hexdigest()

    def _load_used_combinations(self) -> None:
        """Memuat kombinasi yang sudah digunakan dari file JSON."""
        self.used_combinations = set()
        if os.path.exists(self.combinations_json_path):
            try:
                with open(self.combinations_json_path, 'r') as f:
                    data = json.load(f)
                    for entry in data:
                        self.used_combinations.add(entry['hash'])
            except Exception as e:
                print(f"Error memuat kombinasi JSON: {str(e)}")

    def _save_combination(self, combination: Dict) -> None:
        """Menyimpan kombinasi baru ke file JSON."""
        if not self.combinations_json_path:
            return
        try:
            data = []
            if os.path.exists(self.combinations_json_path):
                with open(self.combinations_json_path, 'r') as f:
                    data = json.load(f)
            data.append(combination)
            with open(self.combinations_json_path, 'w') as f:
                json.dump(data, f, indent=4)
        except Exception as e:
            print(f"Error menyimpan kombinasi JSON: {str(e)}")

    def _generate_sample_params(self, delta_E_max_dict: Dict[str, float]) -> List[Tuple[float, float]]:
        """Menghasilkan parameter sampel dengan distribusi suhu seimbang."""
        num_temperatures = len(self.temperature_range)
        samples_per_temp = self.num_samples // num_temperatures
        remainder = self.num_samples % num_temperatures

        temp_specific_params = {T: [] for T in self.temperature_range}
        for i, T in enumerate(self.temperature_range):
            n_samples_for_temp = samples_per_temp + (1 if i < remainder else 0)
            delta_E_values = [v for v in delta_E_max_dict.values() if v > 0]
            delta_E_max_value = max(delta_E_values) if delta_E_values else 4.0
            n_e_min = self._calculate_lte_electron_density(T, delta_E_max_value)
            valid_n_e = [n_e for n_e in self.electron_density_range if n_e >= n_e_min]
            if not valid_n_e:
                print(f"Peringatan: Tidak ada densitas elektron valid untuk T={T} K. Menggunakan seluruh rentang.")
                valid_n_e = self.electron_density_range

            max_ne_per_temp = min(len(valid_n_e), 5)
            if max_ne_per_temp == 0:
                max_ne_per_temp = len(self.electron_density_range)
            selected_n_e = np.random.choice(valid_n_e, size=max_ne_per_temp, replace=False).tolist()

            if len(selected_n_e) < n_samples_for_temp:
                additional_n_e = np.random.choice(selected_n_e, size=n_samples_for_temp - len(selected_n_e), replace=True)
                selected_n_e.extend(additional_n_e)

            temp_specific_params[T].extend([(T, n_e) for n_e in selected_n_e[:n_samples_for_temp]])

        # Seleksi round-robin untuk distribusi merata
        sample_params = []
        indices = {T: 0 for T in self.temperature_range}
        remaining = {T: len(params) for T, params in temp_specific_params.items()}
        total_remaining = sum(remaining.values())

        while total_remaining > 0:
            for T in self.temperature_range:
                if remaining[T] > 0:
                    idx = indices[T]
                    sample_params.append(temp_specific_params[T][idx])
                    indices[T] += 1
                    remaining[T] -= 1
                    total_remaining -= 1

        if len(sample_params) < self.num_samples:
            print(f"Peringatan: Hanya {len(sample_params)} sampel dihasilkan, menggandakan...")
            while len(sample_params) < self.num_samples:
                idx = np.random.randint(0, len(sample_params))
                sample_params.append(sample_params[idx])
        elif len(sample_params) > self.num_samples:
            sample_params = sample_params[:self.num_samples]

        return sample_params

    def generate_dataset(
        self,
        simulators: List[SpectrumSimulator],
        delta_E_max: Dict[str, float],
        ionization_energies: Dict[str, float],
        processed_dir: str,
        drive_processed_dir: str
    ) -> None:
        """Menghasilkan dan menyimpan dataset ke HDF5, meng-append ke sub-grup train/validation/test."""
        # Set seed untuk konsistensi pengacakan
        np.random.seed(42)  # Gunakan seed tetap untuk reproduksibilitas
        self.combinations_json_path = os.path.join(drive_processed_dir, "combinations.json")
        self._load_used_combinations()
        mixed_simulator = MixedSpectrumSimulator(simulators, self.config, delta_E_max)
        spectra_list, labels_list, wavelengths_list, atom_percentages_list = [], [], [], []

        sample_params = self._generate_sample_params(delta_E_max)

        # Debug distribusi akhir
        temps = [param[0] for param in sample_params]
        temp_counts = Counter(temps)
        for temp, count in temp_counts.items():
            print(f"Distribusi akhir - Suhu {temp} K: {count} sampel ({count/self.num_samples*100:.2f}%)")

        for i, (T, n_e) in enumerate(tqdm(sample_params, desc="Menghasilkan dataset")):
            mixed_simulator.current_T = T
            mixed_simulator.current_n_e = n_e
            max_attempts = 5
            attempt = 0
            sample_generated = False

            while attempt < max_attempts and not sample_generated:
                result = mixed_simulator.generate_sample(ionization_energies)
                if result[0] is None:
                    attempt += 1
                    continue

                wavelengths, spectrum, labels, atom_percentages = result
                combination_hash = self._hash_combination(T, n_e, atom_percentages)
                if combination_hash in self.used_combinations:
                    print(f"Kombinasi sudah ada untuk T={T} K, n_e={n_e:.2e} cm^-3, coba ulang ({attempt + 1}/{max_attempts})")
                    attempt += 1
                    continue

                spectra_list.append(spectrum)
                labels_list.append(labels)
                wavelengths_list.append(wavelengths)
                atom_percentages_list.append(atom_percentages)

                # Simpan kombinasi ke JSON
                combination = {
                    'sample_id': f"sample_{len(self.used_combinations) + 1}",
                    'hash': combination_hash,
                    'temperature': float(T),
                    'electron_density': float(n_e),
                    'elements': {k: v for k, v in atom_percentages.items()
                                if k not in ['temperature', 'electron_density', 'delta_E_max']},
                    'delta_E_max': atom_percentages['delta_E_max']
                }
                self._save_combination(combination)
                self.used_combinations.add(combination_hash)
                sample_generated = True

            if not sample_generated:
                print(f"Peringatan: Gagal menghasilkan sampel unik untuk T={T} K, n_e={n_e:.2e} cm^-3 setelah {max_attempts} percobaan")

            if (i + 1) % (self.num_samples // 10) == 0:
                current_temps = [param[0] for param in sample_params[:i + 1]]
                current_counts = Counter(current_temps)
                print(f"Progres {(i + 1)/self.num_samples*100:.1f}%:")
                for temp, count in current_counts.items():
                    print(f"Suhu {temp} K: {count} sampel ({count/(i + 1)*100:.2f}%)")

        if not spectra_list:
            raise ValueError("Tidak ada sampel valid yang dihasilkan. Periksa data NIST atau konfigurasi simulator.")

        spectra_array = np.array(spectra_list, dtype=np.float32)
        labels_array = np.array(labels_list, dtype=np.int32)
        wavelengths_array = np.array(wavelengths_list[0], dtype=np.float32)  # Ambil wavelengths pertama (seharusnya sama)
        atom_percentages_array = [json.dumps(d).encode('utf-8') for d in atom_percentages_list]

        # Bagi data ke train, validation, dan test
        actual_samples = len(spectra_list)
        num_train = int(0.7 * actual_samples)
        num_val = int(0.15 * actual_samples)
        num_test = actual_samples - num_train - num_val

        indices = np.random.permutation(actual_samples)
        train_idx = indices[:num_train]
        val_idx = indices[num_train:num_train + num_val]
        test_idx = indices[num_train + num_val:]

        train_data = (
            spectra_array[train_idx],
            labels_array[train_idx],
            [atom_percentages_array[i] for i in train_idx]
        )
        val_data = (
            spectra_array[val_idx],
            labels_array[val_idx],
            [atom_percentages_array[i] for i in val_idx]
        )
        test_data = (
            spectra_array[test_idx],
            labels_array[test_idx],
            [atom_percentages_array[i] for i in test_idx]
        )

        # Simpan ke HDF5 di drive_processed_dir
        output_filename = "spectral_dataset.h5"
        drive_output_path = os.path.join(drive_processed_dir, output_filename)
        os.makedirs(drive_processed_dir, exist_ok=True)

        # Buat backup jika file sudah ada
        if os.path.exists(drive_output_path):
            backup_path = os.path.join(drive_processed_dir, f"spectral_dataset_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.h5")
            shutil.copy(drive_output_path, backup_path)
            print(f"Backup dibuat di {backup_path}")

        # Baca data yang sudah ada (jika ada)
        existing_data = {'train': {'spectra': [], 'labels': [], 'atom_percentages': []},
                        'validation': {'spectra': [], 'labels': [], 'atom_percentages': []},
                        'test': {'spectra': [], 'labels': [], 'atom_percentages': []}}
        total_existing_samples = 0

        if os.path.exists(drive_output_path):
            with h5py.File(drive_output_path, 'r') as f:
                # Validasi wavelengths jika ada
                if 'wavelengths' in f:
                    existing_wavelengths = f['wavelengths'][:]
                    if not np.array_equal(existing_wavelengths, wavelengths_array):
                        raise ValueError("Wavelengths baru tidak cocok dengan wavelengths yang sudah ada")

                for ds_name in ['train', 'validation', 'test']:
                    if ds_name in f:
                        if 'spectra' in f[ds_name]:
                            existing_data[ds_name]['spectra'] = f[ds_name]['spectra'][:]
                        if 'labels' in f[ds_name]:
                            existing_data[ds_name]['labels'] = f[ds_name]['labels'][:]
                        if 'atom_percentages' in f[ds_name]:
                            existing_data[ds_name]['atom_percentages'] = f[ds_name]['atom_percentages'][:]
                        total_existing_samples += len(existing_data[ds_name]['spectra'])

        # Gabungkan data baru dengan data lama
        # Gabungkan data baru dengan data lama
            # Gabungkan data baru dengan data lama
        combined_data = {}
        for ds_name, new_data in [('train', train_data), ('validation', val_data), ('test', test_data)]:
            new_spectra, new_labels, new_atom_percentages = new_data
            existing_spectra = existing_data[ds_name]['spectra']
            existing_labels = existing_data[ds_name]['labels']
            existing_atom_percentages = existing_data[ds_name]['atom_percentages']

            combined_spectra = np.concatenate([existing_spectra, new_spectra]) if len(existing_spectra) > 0 else new_spectra
            combined_labels = np.concatenate([existing_labels, new_labels]) if len(existing_labels) > 0 else new_labels
            # Convert existing_atom_percentages to list if it's a NumPy array
            existing_atom_percentages_list = existing_atom_percentages.tolist() if isinstance(existing_atom_percentages, np.ndarray) else existing_atom_percentages
            combined_atom_percentages = existing_atom_percentages_list + new_atom_percentages if len(existing_atom_percentages_list) > 0 else new_atom_percentages

            combined_data[ds_name] = (combined_spectra, combined_labels, combined_atom_percentages)

        # Simpan ke HDF5 dengan mode 'a'
        with h5py.File(drive_output_path, 'a') as f:
            # Hapus sub-grup lama jika ada
            for ds_name in ['train', 'validation', 'test']:
                if ds_name in f:
                    del f[ds_name]

            # Simpan wavelengths di root
            if 'wavelengths' in f:
                del f['wavelengths']
            f.create_dataset('wavelengths', data=wavelengths_array)

            # Simpan data gabungan
            total_samples = 0
            for ds_name, (spectra, labels, atom_percentages) in combined_data.items():
                ds_grp = f.create_group(ds_name)
                ds_grp.create_dataset('spectra', data=spectra, compression='gzip')
                ds_grp.create_dataset('labels', data=labels, compression='gzip')
                ds_grp.create_dataset('atom_percentages', data=atom_percentages, compression='gzip')
                total_samples += len(spectra)

            # Perbarui atribut root
            f.attrs['last_updated'] = datetime.now().isoformat()
            f.attrs['total_samples'] = total_samples
            f.attrs['simulation_config'] = json.dumps(self.config)

        print(f"Dataset di-append ke {drive_output_path} di sub-grup train/validation/test")
        print(f"Total sampel: {total_samples} (baru: {actual_samples}, lama: {total_existing_samples})")


class DataManager:
    """Mengelola pemuatan data, validasi, dan operasi file."""

    def __init__(self, drive_base_dir: str):
        self.drive_base_dir = drive_base_dir
        self.data_dir = os.path.join(drive_base_dir, "data/raw/HDF5")
        self.processed_dir = "/content"
        self.drive_processed_dir = os.path.join(drive_base_dir, "data/processed")
        self.nist_source_path = os.path.join(self.data_dir, "nist_data(1).h5")
        self.nist_target_path = "/content/nist_data(1).h5"
        self.atomic_data_source_path = os.path.join(self.data_dir, "atomic_data1.h5")
        self.atomic_data_target_path = "/content/atomic_data1.h5"
        self.json_map_path = os.path.join(self.drive_base_dir, "data/processed", "element_map.json")

    def mount_drive(self) -> None:
        """Mount Google Drive."""
        mountpoint = '/content/drive'
        try:
            drive.flush_and_unmount()
        except ValueError:
            pass
        os.makedirs(mountpoint, exist_ok=True)
        drive.mount(mountpoint, force_remount=True)

    def copy_files(self) -> None:
        """Menyalin file yang diperlukan dari Drive ke lokal."""
        for source, target in tqdm([(self.nist_source_path, self.nist_target_path),
                                  (self.atomic_data_source_path, self.atomic_data_target_path)],
                                 desc="Menyalin file"):
            if not os.path.exists(source):
                raise FileNotFoundError(f"File tidak ditemukan di {source}")
            shutil.copy(source, target)

    def load_element_map(self) -> Dict:
        """Memuat dan memvalidasi peta elemen."""
        if not os.path.exists(self.json_map_path):
            raise FileNotFoundError(f"element_map.json tidak ditemukan di {self.json_map_path}")
        with open(self.json_map_path, 'r') as f:
            element_map = json.load(f)

        for elem in REQUIRED_ELEMENTS:
            if elem not in element_map or not isinstance(element_map[elem], list) or abs(sum(element_map[elem]) - 1.0) > 1e-6:
                raise ValueError(f"Peta elemen tidak valid untuk {elem}: harus berupa daftar dengan jumlah 1.0")

        return element_map

    def load_ionization_energies(self) -> Dict[str, float]:
        """Memuat energi ionisasi dari data atom."""
        ionization_energies = {}
        try:
            with h5py.File(self.atomic_data_target_path, 'r') as f:
                dset = f['elements']
                columns = dset.attrs.get('columns', ['At. num', 'Sp. Name', 'Ion Charge', 'El. Name',
                                                   'Prefix', 'Ionization Energy (eV)', 'Suffix'])
                if 'columns' not in dset.attrs:
                    print("Peringatan: Atribut 'columns' tidak ada. Menggunakan kolom default.")

                data = [[item[0], item[1].decode('utf-8'), item[2].decode('utf-8'), item[3].decode('utf-8'),
                        item[4].decode('utf-8'), item[5], item[6].decode('utf-8')] for item in dset[:]]
                df_ionization = pd.DataFrame(data, columns=columns)

                species_col = ion_energy_col = None
                for col in columns:
                    if col.lower() in ['sp.', 'species', 'sp', 'element', 'sp. name']:
                        species_col = col
                    if 'ionization' in col.lower() and 'ev' in col.lower():
                        ion_energy_col = col

                if not species_col or not ion_energy_col:
                    raise KeyError(f"Kolom yang diperlukan tidak ditemukan. Tersedia: {list(df_ionization.columns)}")

                for _, row in df_ionization.iterrows():
                    try:
                        ionization_energies[row[species_col]] = float(row[ion_energy_col])
                    except (ValueError, TypeError):
                        print(f"Peringatan: Energi ionisasi tidak valid untuk {row[species_col]}, menggunakan 0.0 eV")
                        ionization_energies[row[species_col]] = 0.0

        except Exception as e:
            print(f"Error memuat atomic_data1.h5: {str(e)}")
            raise

        for elem in REQUIRED_ELEMENTS:
            base_elem, ion = elem.split('_')
            ion_level = 'I' if ion == '1' else 'II'
            sp_name = f"{base_elem} {ion_level}"
            if sp_name not in ionization_energies:
                print(f"Peringatan: Tidak ada energi ionisasi untuk {sp_name}, menggunakan 0.0 eV")
                ionization_energies[sp_name] = 0.0

        return ionization_energies

def main():
    """Fungsi utama untuk menjalankan simulasi dan menghasilkan dataset."""
    pd.set_option('future.no_silent_downcasting', True)

    data_manager = DataManager("/content/drive/MyDrive/libs_lstm")

    # Mount Drive dan salin file
    data_manager.mount_drive()
    data_manager.copy_files()

    # Muat peta elemen dan energi ionisasi
    element_map = data_manager.load_element_map()
    ionization_energies = data_manager.load_ionization_energies()

    # Muat data NIST
    fetcher = DataFetcher(data_manager.nist_target_path)
    nist_data_dict = {}
    delta_E_max_dict = {}
    for elem in REQUIRED_ELEMENTS:
        element, ion = elem.split('_')
        data, delta_E = fetcher.get_nist_data(element, int(ion))
        nist_data_dict[elem] = data
        delta_E_max_dict[elem] = delta_E
        if not data:
            print(f"Tidak ada data NIST untuk {elem}")

    # Buat simulator
    simulators = []
    for elem in nist_data_dict:
        if nist_data_dict[elem]:
            element, ion = elem.split('_')
            ion_energy = ionization_energies.get(f"{element} {'I' if int(ion) == 1 else 'II'}", 0.0)
            simulator = SpectrumSimulator(
                nist_data_dict[elem],
                element,
                int(ion),
                SIMULATION_CONFIG["temperature_range"],
                ion_energy,
                SIMULATION_CONFIG
            )
            simulators.append(simulator)

    if not simulators:
        raise ValueError("Tidak ada simulator valid yang dibuat. Periksa data NIST.")

    # Hasilkan dataset
    generator = DatasetGenerator(SIMULATION_CONFIG)
    generator.generate_dataset(
        simulators,
        delta_E_max_dict,
        ionization_energies,
        data_manager.processed_dir,
        data_manager.drive_processed_dir
    )

if __name__ == "__main__":
    main()

# Sel untuk memeriksa versi library di Google Colab

# Impor library yang diperlukan
import os
import numpy as np
import pandas as pd
import json
import re
import torch
import torch.nn.functional as F
from scipy.signal.windows import gaussian
import h5py
from tqdm import tqdm
import shutil
from google.colab import drive
from collections import Counter
import hashlib
from datetime import datetime
import sys
from typing import get_type_hints  # Untuk memeriksa typing

# Fungsi untuk mencetak versi library
def print_library_versions():
    print("Versi Library yang Digunakan di Google Colab:")
    print("-" * 50)

    # Modul standar Python (tanpa versi spesifik, hanya versi Python)
    print(f"Python: {sys.version}")
    print("os: Modul standar Python")
    print("json: Modul standar Python")
    print("re: Modul standar Python")
    print("shutil: Modul standar Python")
    print("collections: Modul standar Python")
    print("hashlib: Modul standar Python")
    print("datetime: Modul standar Python")
    print("typing: Modul standar Python")

    # Library pihak ketiga
    try:
        print(f"numpy: {np.__version__}")
    except AttributeError:
        print("numpy: Tidak terdeteksi")

    try:
        print(f"pandas: {pd.__version__}")
    except AttributeError:
        print("pandas: Tidak terdeteksi")

    try:
        print(f"torch: {torch.__version__}")
    except AttributeError:
        print("torch: Tidak terdeteksi")

    try:
        from scipy import __version__ as scipy_version
        print(f"scipy: {scipy_version}")
    except ImportError:
        print("scipy: Tidak terdeteksi")

    try:
        print(f"h5py: {h5py.__version__}")
    except AttributeError:
        print("h5py: Tidak terdeteksi")

    try:
        print(f"tqdm: {tqdm.__version__}")
    except AttributeError:
        print("tqdm: Tidak terdeteksi")

    # Google Colab spesifik
    try:
        import google.colab
        # google.colab tidak memiliki versi eksplisit, tetapi kita bisa cek versi Colab
        print("google.colab: Tersedia (versi spesifik tidak dilaporkan oleh library)")
    except ImportError:
        print("google.colab: Tidak terdeteksi")

# Jalankan fungsi
print_library_versions()

import h5py
import json
import numpy as np
from collections import Counter
from typing import Dict, List, Tuple
import os
from datetime import datetime

def analyze_hdf5_dataset(hdf5_path: str, output_dir: str) -> None:
    """
    Menganalisis isi file HDF5 spectral_dataset.h5 dan menyimpan hasilnya ke file teks.

    Args:
        hdf5_path (str): Jalur ke file HDF5 (misalnya, spectral_dataset.h5).
        output_dir (str): Direktori untuk menyimpan hasil analisis.
    """
    # Pastikan direktori output ada
    os.makedirs(output_dir, exist_ok=True)

    # Nama file output dengan timestamp
    output_filename = f"dataset_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    output_path = os.path.join(output_dir, output_filename)

    # Inisialisasi untuk menyimpan statistik
    total_samples = 0
    temp_dist = Counter()
    element_dist = Counter()
    dataset_info = {
        'train_samples': 0,
        'val_samples': 0,
        'test_samples': 0,
        'temperatures': Counter(),
        'elements': Counter(),
        'last_updated': 'Unknown',
        'total_samples_attr': 0,
        'simulation_config': {}
    }

    with h5py.File(hdf5_path, 'r') as f:
        # Baca atribut root
        dataset_info['last_updated'] = f.attrs.get('last_updated', 'Unknown')
        dataset_info['total_samples_attr'] = f.attrs.get('total_samples', 0)
        try:
            dataset_info['simulation_config'] = json.loads(f.attrs.get('simulation_config', '{}'))
        except json.JSONDecodeError:
            dataset_info['simulation_config'] = {}

        # Baca wavelengths
        wavelengths_shape = f['wavelengths'].shape if 'wavelengths' in f else (0,)

        # Analisis sub-grup (train, validation, test)
        for ds_name in ['train', 'validation', 'test']:
            if ds_name in f:
                ds_group = f[ds_name]
                spectra = ds_group.get('spectra', None)
                atom_percentages = ds_group.get('atom_percentages', None)

                if spectra is not None:
                    num_samples = spectra.shape[0]
                    dataset_info[f'{ds_name}_samples'] = num_samples
                    total_samples += num_samples

                    # Analisis atom_percentages untuk distribusi suhu dan elemen
                    if atom_percentages is not None:
                        for ap in atom_percentages:
                            ap_dict = json.loads(ap.decode('utf-8'))
                            temp = ap_dict.get('temperature', 0.0)
                            dataset_info['temperatures'][temp] += 1
                            for elem, percentage in ap_dict.items():
                                if elem not in ['temperature', 'electron_density', 'delta_E_max'] and percentage > 0:
                                    dataset_info['elements'][elem] += 1

        temp_dist.update(dataset_info['temperatures'])
        element_dist.update(dataset_info['elements'])

    # Tulis hasil analisis ke file teks
    with open(output_path, 'w') as f:
        f.write(f"Analisis Dataset HDF5: {hdf5_path}\n")
        f.write(f"Tanggal dan Waktu: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write(f"Total Sampel (dihitung): {total_samples}\n")
        f.write(f"Total Sampel (dari atribut): {dataset_info['total_samples_attr']}\n")
        f.write(f"Terakhir Diperbarui: {dataset_info['last_updated']}\n")
        f.write(f"Konfigurasi Simulasi: {json.dumps(dataset_info['simulation_config'], indent=2)}\n")
        f.write(f"Dimensi Wavelengths: {wavelengths_shape}\n\n")

        f.write("Detail Dataset:\n")
        f.write("-" * 50 + "\n")
        f.write(f"Sampel Train: {dataset_info['train_samples']}\n")
        f.write(f"Sampel Validation: {dataset_info['val_samples']}\n")
        f.write(f"Sampel Test: {dataset_info['test_samples']}\n")
        f.write("Distribusi Suhu:\n")
        for temp, count in sorted(dataset_info['temperatures'].items()):
            f.write(f"  {temp} K: {count} sampel ({count/total_samples*100:.2f}%)\n")
        f.write("Elemen yang Digunakan:\n")
        for elem, count in sorted(dataset_info['elements'].items()):
            f.write(f"  {elem}: {count} kali\n")
        f.write("-" * 50 + "\n")

    print(f"Hasil analisis disimpan ke {output_path}")
def main():
    # Konfigurasi jalur
    drive_base_dir = "/content/drive/MyDrive/libs_lstm"
    hdf5_path = os.path.join(drive_base_dir, "data/processed", "spectral_dataset.h5")
    output_dir = os.path.join(drive_base_dir, "data/processed/analysis")

    # Jalankan analisis
    analyze_hdf5_dataset(hdf5_path, output_dir)

if __name__ == "__main__":
    main()

# prompt: buat kode untuk yang ada pake yg lain dari kode orang lain

import pandas as pd
import numpy as np
#Versi Stabil Multi-elemen Stratifikasi Suhu Densitas

# Simulation configuration
SIMULATION_CONFIG = {
    "resolution": 4096,
    "wl_range": (200, 900),
    "sigma": 0.1,  # nm, for Gaussian broadening
    "target_max_intensity": 0.8,
    "convolution_sigma": 0.1,  # nm, for spectrum convolution
    "num_samples": 10000,
    "temperature_range": [6000, 8000, 10000, 12000, 14000, 15000],  # K
    "electron_density_range": np.logspace(15, 17, 10).tolist(),  # cm^-3, from 1e15 to 5e17
}

# Physical constants
PHYSICAL_CONSTANTS = {
    "k_B": 8.617333262145e-5,  # eV/K
    "m_e": 9.1093837e-31,      # kg
    "h": 4.135667696e-15,      # eV·s
}

# Elements and ions
BASE_ELEMENTS = ["Si", "Al", "Fe", "Ca", "O", "Na", "N", "Ni",  "Cr", "Cl"]
REQUIRED_ELEMENTS = [f"{elem}_{ion}" for elem in BASE_ELEMENTS for ion in [1, 2]]

def calculate_lte_electron_density(temp: float, delta_E: float) -> float:
    """Calculate minimum electron density for LTE based on temperature and Delta E.

    Args:
        temp: Temperature in Kelvin.
        delta_E: Transition energy difference in eV.

    Returns:
        Minimum electron density in cm^-3.
    """
    return 1.6e12 * (temp ** 0.5) * (delta_E ** 3)

def get_valid_combinations(temperature_range: List[float], electron_density_range: List[float], delta_E_max_dict: Dict[str, float]) -> List[Tuple[float, float]]:
    """Generate valid temperature and electron density combinations based on LTE constraints.

    Args:
        temperature_range: List of temperatures in Kelvin.
        electron_density_range: List of electron density in cm^-3.
        delta_E_max_dict: Dictionary of maximum Delta E per element and ion.

    Returns:
        List of valid (temperature, electron_density) tuples.
    """
    combinations = []
    for T in temperature_range:
        delta_E_values = [v for v in delta_E_max_dict.values() if v > 0]
        delta_E_max = max(delta_E_values) if delta_E_values else 4.0
        n_e_min = calculate_lte_electron_density(T, delta_E_max)
        valid_n_e = [n_e for n_e in electron_density_range if n_e >= n_e_min]
        for n_e in valid_n_e:
            combinations.append((T, n_e))
    return combinations

class DataFetcher:
    """Fetch spectral data from NIST HDF5 file for specific elements and ions."""

    def __init__(self, hdf_path: str):
        self.hdf_path = hdf_path
        self.delta_E_max = {}  # Cache maximum Delta E per element and ion

    def get_nist_data(self, element: str, sp_num: int) -> Tuple[List[List], float]:
        """Fetch spectral data for an element and ion, and calculate maximum Delta E.

        Args:
            element: Element symbol (e.g., 'Si').
            sp_num: Ionization number (1 for neutral, 2 for +1 ion).

        Returns:
            Tuple of spectral data list (wavelength, Aki, Ek, Ei, g_i, g_k, Acc)
            and maximum Delta E in eV. Returns ([], 0.0) if failed.
        """
        try:
            with pd.HDFStore(self.hdf_path, mode='r') as store:
                df = store.get('nist_spectroscopy_data')
                filtered_df = df[(df['element'] == element) & (df['sp_num'] == sp_num)]
                required_columns = ['ritz_wl_air(nm)', 'Aki(s^-1)', 'Ek(eV)', 'Ei(eV)', 'g_i', 'g_k']

                if filtered_df.empty or not all(col in df.columns for col in required_columns):
                    print(f"No data found for {element}_{sp_num} in NIST dataset")
                    return [], 0.0

                filtered_df = filtered_df.dropna(subset=required_columns)

                # Convert columns to numeric
                filtered_df['ritz_wl_air(nm)'] = pd.to_numeric(filtered_df['ritz_wl_air(nm)'], errors='coerce')
                for col in ['Ek(eV)', 'Ei(eV)', 'Aki(s^-1)', 'g_i', 'g_k']:
                    filtered_df[col] = pd.to_numeric(
                        filtered_df[col].apply(lambda x: re.sub(r'[^\d.-]', '', str(x)) if re.sub(r'[^\d.-]', '', str(x)) else None),
                        errors='coerce'
                    )

                filtered_df = filtered_df.dropna(subset=['ritz_wl_air(nm)', 'Ek(eV)', 'Ei(eV)', 'Aki(s^-1)', 'g_i', 'g_k'])

                # Filter wavelength range
                filtered_df = filtered_df[
                    (filtered_df['ritz_wl_air(nm)'] >= SIMULATION_CONFIG["wl_range"][0]) &
                    (filtered_df['ritz_wl_air(nm)'] <= SIMULATION_CONFIG["wl_range"][1])
                ]

                # Calculate Delta E (eV)
                filtered_df['delta_E'] = abs(filtered_df['Ek(eV)'] - filtered_df['Ei(eV)'])
                if filtered_df.empty:
                    print(f"No valid transitions for {element}_{sp_num} in wavelength range")
                    return [], 0.0

                filtered_df = filtered_df.sort_values(by='Aki(s^-1)', ascending=False)
                delta_E_max = filtered_df['delta_E'].max()
                delta_E_max = 0.0 if pd.isna(delta_E_max) else delta_E_max
                self.delta_E_max[f"{element}_{sp_num}"] = delta_E_max

                return filtered_df[required_columns + ['Acc']].values.tolist(), delta_E_max
        except Exception as e:
            print(f"Error fetching NIST data for {element}_{sp_num}: {str(e)}")
            return [], 0.0

class SpectrumSimulator:
    """Simulate emission spectra for a single element and ion at various temperatures."""

    def __init__(
        self,
        nist_data: List[List],
        element: str,
        ion: int,
        temperatures: List[float],
        ionization_energy: float,
        config: Dict = SIMULATION_CONFIG
    ):
        self.nist_data = nist_data
        self.element = element
        self.ion = ion
        self.temperatures = temperatures
        self.ionization_energy = ionization_energy
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.sigma = config["sigma"]
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.gaussian_cache = {}
        self.element_label = f"{element}_{ion}"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def partition_function(self, energy_levels: List[float], degeneracies: List[float], temperature: float) -> float:
        """Calculate partition function for energy levels at a given temperature."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return sum(g * np.exp(-E / (k_B * temperature)) for g, E in zip(degeneracies, energy_levels) if E is not None) or 1.0

    def calculate_intensity(self, temperature: float, energy: float, degeneracy: float, einstein_coeff: float, Z: float) -> float:
        """Calculate spectral line intensity under LTE conditions."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        return (degeneracy * einstein_coeff * np.exp(-energy / (k_B * temperature))) / Z

    def gaussian_profile(self, center: float) -> np.ndarray:
        """Generate Gaussian profile for a center wavelength."""
        if center not in self.gaussian_cache:
            x_tensor = torch.tensor(self.wavelengths, device=self.device, dtype=torch.float32)
            center_tensor = torch.tensor(center, device=self.device, dtype=torch.float32)
            sigma_tensor = torch.tensor(self.sigma, device=self.device, dtype=torch.float32)
            gaussian = torch.exp(-0.5 * ((x_tensor - center_tensor) / sigma_tensor) ** 2) / (sigma_tensor * torch.sqrt(torch.tensor(2 * np.pi)))
            self.gaussian_cache[center] = gaussian.cpu().numpy().astype(np.float32)
        return self.gaussian_cache[center]

    def simulate(self, atom_percentage: float = 1.0) -> Tuple[np.ndarray, List[np.ndarray], List[List[int]], List[List[int]], List[float], List[List[Dict]], List[np.ndarray]]:
        """Simulate spectra for all given temperatures.

        Returns:
            Tuple of wavelengths, spectra, peak indices, peak labels,
            temperatures, intensity data, and element contributions.
        """
        spectra, peak_indices, peak_labels, temperatures, intensity_data, contributions = [], [], [], [], [], []
        levels = {}

        for data in self.nist_data:
            try:
                wl, Aki, Ek, Ei, gi, gk, _ = data
                if all(v is not None for v in [wl, Aki, Ek, Ei, gi, gk]):
                    Ek = float(Ek)
                    Ei = float(Ei)
                    if Ei not in levels:
                        levels[Ei] = float(gi)
                    if Ek not in levels:
                        levels[Ek] = float(gk)
            except (ValueError, TypeError):
                continue

        if not levels:
            print(f"Warning: No valid energy levels for {self.element_label}")
            return self.wavelengths, [], [], [], [], [], []

        energy_levels = list(levels.keys())
        degeneracies = list(levels.values())

        for temp in self.temperatures:
            Z = self.partition_function(energy_levels, degeneracies, temp)
            intensities = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
            element_contributions = torch.zeros(self.resolution, device=self.device, dtype=torch.float32)
            peak_idx, peak_label, temp_intensity_data = [], [], []

            for data in self.nist_data:
                try:
                    wl, Aki, Ek, Ei, gi, gk, _ = data
                    if all(v is not None for v in [wl, Aki, Ek, Ei, gi, gk]):
                        wl = float(wl)
                        Aki = float(Aki)
                        Ek = float(Ek)
                        intensity = self.calculate_intensity(temp, Ek, float(gk), Aki, Z)
                        idx = np.searchsorted(self.wavelengths, wl)
                        if 0 <= idx < self.resolution:
                            gaussian_contribution = torch.tensor(
                                intensity * atom_percentage * self.gaussian_profile(wl),
                                device=self.device,
                                dtype=torch.float32
                            )
                            start_idx = max(0, idx - len(gaussian_contribution) // 2)
                            end_idx = min(self.resolution, start_idx + len(gaussian_contribution))
                            if start_idx < end_idx:
                                intensities[start_idx:end_idx] += gaussian_contribution[:end_idx - start_idx]
                                element_contributions[start_idx:end_idx] += gaussian_contribution[:end_idx - start_idx]
                            temp_intensity_data.append({
                                'wavelength': wl,
                                'intensity': intensity * atom_percentage,
                                'element_label': self.element_label,
                                'index': idx
                            })
                            class_idx = REQUIRED_ELEMENTS.index(self.element_label) if self.element_label in REQUIRED_ELEMENTS else len(REQUIRED_ELEMENTS)
                            peak_idx.append(idx)
                            peak_label.append(class_idx)
                except (ValueError, TypeError):
                    continue

            spectra.append(intensities.cpu().numpy())
            peak_indices.append(peak_idx)
            peak_labels.append(peak_label)
            temperatures.append(temp)
            intensity_data.append(temp_intensity_data)
            contributions.append(element_contributions.cpu().numpy())

        return self.wavelengths, spectra, peak_indices, peak_labels, temperatures, intensity_data, contributions

class MixedSpectrumSimulator:
    """Combine spectra from multiple elements based on atomic proportions."""

    def __init__(self, simulators: List[SpectrumSimulator], config: Dict, delta_E_max: Dict[str, float]):
        self.simulators = simulators
        self.resolution = config["resolution"]
        self.wl_range = config["wl_range"]
        self.convolution_sigma = config["convolution_sigma"]
        self.electron_density_range = config["electron_density_range"]
        self.delta_E_max = delta_E_max
        self.wavelengths = np.linspace(self.wl_range[0], self.wl_range[1], self.resolution, dtype=np.float32)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.intensity_threshold = 0.01
        self.current_T: float = 0.0
        self.current_n_e: float = 0.0

    def _normalize_intensity(self, intensity: np.ndarray, target_max: float) -> np.ndarray:
        """Normalize spectrum intensity to target maximum."""
        intensity_tensor = torch.tensor(intensity, device=self.device, dtype=torch.float32)
        max_intensity = torch.max(torch.abs(intensity_tensor))
        if max_intensity == 0:
            return intensity
        return (intensity_tensor / max_intensity * target_max).cpu().numpy()

    def _convolve_spectrum(self, spectrum: np.ndarray, sigma_nm: float) -> np.ndarray:
        """Convolve spectrum with Gaussian kernel."""
        spectrum_tensor = torch.tensor(spectrum, dtype=torch.float32, device=self.device)
        wavelength_step = (self.wavelengths[-1] - self.wavelengths[0]) / (len(self.wavelengths) - 1)
        sigma_points = sigma_nm / wavelength_step
        kernel_size = int(6 * sigma_points) | 1
        kernel = torch.tensor(
            gaussian(kernel_size, sigma_points) / np.sum(gaussian(kernel_size, sigma_points)),
            device=self.device,
            dtype=torch.float32
        )
        kernel = kernel.unsqueeze(0).unsqueeze(0)
        spectrum_tensor = spectrum_tensor.unsqueeze(0).unsqueeze(0)
        convolved = F.conv1d(spectrum_tensor, kernel, padding=kernel_size//2).squeeze().cpu().numpy()
        return convolved.astype(np.float32)

    def _saha_ratio(self, ion_energy: float, temp: float, electron_density: float) -> float:
        """Calculate Saha ratio for ionization."""
        k_B = PHYSICAL_CONSTANTS["k_B"]
        m_e = PHYSICAL_CONSTANTS["m_e"]
        h = PHYSICAL_CONSTANTS["h"]
        two_pi_me_kT_h2 = (2 * np.pi * m_e * (k_B * temp * 1.60217662e-16) / (h * 1.60217662e-19) ** 2) ** (3/2)
        two_pi_me_kT_h2 /= 1e6
        U_i = 1.0
        U_ip1 = 1.0
        saha_factor = (2 * U_ip1 / U_i) * two_pi_me_kT_h2 / electron_density
        return saha_factor * np.exp(-ion_energy / (k_B * temp))

    def generate_sample(self, ionization_energies: Dict[str, float]) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray], Optional[Dict]]:
        """Generate a single mixed spectrum sample with specified temperature and electron density."""
        temp = self.current_T
        electron_density = self.current_n_e

        # Select elements randomly
        num_target_elements = 7
        selected_base_elements = np.random.choice(BASE_ELEMENTS, num_target_elements, replace=False)
        selected_pairs = [(elem, f"{elem}_1", f"{elem}_2") for elem in selected_base_elements]

        # Calculate maximum Delta E
        delta_E_values = []
        for base_elem, elem_neutral, elem_ion in selected_pairs:
            for elem in [elem_neutral, elem_ion]:
                delta_E = self.delta_E_max.get(elem, 0.0)
                if delta_E > 0.0:
                    delta_E_values.append(delta_E)
        delta_E_max = max(delta_E_values) if delta_E_values else 4.0

        # Warn about self-absorption
        if electron_density > 5e16 and temp < 8000:
            print(f"Warning: High n_e ({electron_density:.2e} cm^-3) and low T ({temp} K) may cause self-absorption.")

        # Calculate atomic fractions using Saha ratio
        atom_percentages_dict = {}
        total_target_percentage = 0.0
        for base_elem, elem_neutral, elem_ion in selected_pairs:
            ion_energy = ionization_energies.get(f"{base_elem} I", 0.0)
            if ion_energy == 0.0:
                print(f"Warning: No ionization energy for {base_elem} I")
                continue
            saha_ratio = self._saha_ratio(ion_energy, temp, electron_density)
            total_percentage = np.random.uniform(5, 20)
            fraction_neutral = 1 / (1 + saha_ratio)
            fraction_ion = saha_ratio / (1 + saha_ratio)
            percentage_neutral = total_percentage * fraction_neutral
            percentage_ion = total_percentage * fraction_ion
            atom_percentages_dict[elem_neutral] = percentage_neutral / 100.0
            atom_percentages_dict[elem_ion] = percentage_ion / 100.0
            total_target_percentage += total_percentage

        # Scale percentages to sum to 100%
        if total_target_percentage != 0:
            scaling_factor = 100.0 / total_target_percentage
            for key in atom_percentages_dict:
                atom_percentages_dict[key] *= scaling_factor
        else:
            return None, None, None, None

        atom_percentages_dict['temperature'] = float(temp)
        atom_percentages_dict['electron_density'] = float(electron_density)
        atom_percentages_dict['delta_E_max'] = float(delta_E_max)

        selected_target_elements = [k for k in atom_percentages_dict.keys() if k not in ['temperature', 'electron_density', 'delta_E_max']]
        atom_percentages = np.array([atom_percentages_dict[elem] for elem in selected_target_elements], dtype=np.float32)
        selected_simulators = [sim for sim in self.simulators if f"{sim.element}_{sim.ion}" in selected_target_elements]

        if not selected_simulators:
            print(f"Warning: No valid simulators for {selected_target_elements}")
            return None, None, None, None

        mixed_spectrum = np.zeros(self.resolution, dtype=np.float32)
        element_contributions = np.zeros((len(selected_simulators), self.resolution), dtype=np.float32)

        for sim_idx, simulator in enumerate(selected_simulators):
            idx = selected_target_elements.index(f"{simulator.element}_{simulator.ion}")
            atom_percentage = atom_percentages[idx]
            wavelengths, element_spectra, _, _, temps, _, contributions = simulator.simulate(atom_percentage)
            for spectrum, t, contrib in zip(element_spectra, temps, contributions):
                if t == temp:
                    mixed_spectrum += spectrum
                    element_contributions[sim_idx] = contrib
                    break

        if np.max(mixed_spectrum) == 0:
            print(f"Warning: No spectrum generated for T={temp} K")
            return None, None, None, None

        convolved_spectrum = self._convolve_spectrum(mixed_spectrum, self.convolution_sigma)
        normalized_spectrum = self._normalize_intensity(convolved_spectrum, SIMULATION_CONFIG["target_max_intensity"])

        labels = np.zeros(self.resolution, dtype=np.int32)
        contributions_array = np.zeros((self.resolution, len(REQUIRED_ELEMENTS)), dtype=np.float32)

        for idx in range(self.resolution):
            contributions_at_point = element_contributions[:, idx]
            total_intensity = np.sum(contributions_at_point)
            if total_intensity >= self.intensity_threshold:
                dominant_sim_idx = np.argmax(contributions_at_point)
                dominant_element = selected_simulators[dominant_sim_idx].element_label
                if dominant_element in REQUIRED_ELEMENTS:
                    dominant_label = REQUIRED_ELEMENTS.index(dominant_element)
                    labels[idx] = dominant_label + 1
                    contributions_array[idx, dominant_label] = normalized_spectrum[idx]

        atom_percentages_dict = {
            k: float(v * 100) if k not in ['temperature', 'electron_density', 'delta_E_max'] else float(v)
            for k, v in atom_percentages_dict.items()
        }
        return self.wavelengths, normalized_spectrum, labels, atom_percentages_dict

class DatasetGenerator:
    """Generate and save spectral dataset with uniform temperature distribution."""

    def __init__(self, config: Dict = SIMULATION_CONFIG):
        self.config = config
        self.temperature_range = config["temperature_range"]
        self.electron_density_range = config["electron_density_range"]
        self.num_samples = config["num_samples"]
        self.combinations_json_path = None
        self.used_combinations = set()

    def _calculate_lte_electron_density(self, temp: float, delta_E: float) -> float:
        """Calculate minimum electron density for LTE based on temperature and Delta E."""
        return 1.6e12 * (temp ** 0.5) * (delta_E ** 3)

    def _hash_combination(self, temp: float, n_e: float, atom_percentages: Dict) -> str:
        """Create a unique hash for the combination of temperature, electron density, and atomic percentages."""
        elements_sorted = sorted(
            [(k, round(v, 6)) for k, v in atom_percentages.items()
             if k not in ['temperature', 'electron_density', 'delta_E_max']],
            key=lambda x: x[0]
        )
        combination_str = f"{temp:.2f}_{n_e:.2e}_{str(elements_sorted)}"
        return hashlib.sha256(combination_str.encode()).hexdigest()

    def _load_used_combinations(self) -> None:
        """Load already used combinations from a JSON file."""
        self.used_combinations = set()
        if os.path.exists(self.combinations_json_path):
            try:
                with open(self.combinations_json_path, 'r') as f:
                    data = json.load(f)
                    for entry in data:
                        self.used_combinations.add(entry['hash'])
            except Exception as e:
                print(f"Error loading JSON combinations: {str(e)}")

    def _save_combination(self, combination: Dict) -> None:
        """Save a new combination to the JSON file."""
        if not self.combinations_json_path:
            return
        try:
            data = []
            if os.path.exists(self.combinations_json_path):
                with open(self.combinations_json_path, 'r') as f:
                    data = json.load(f)
            data.append(combination)
            with open(self.combinations_json_path, 'w') as f:
                json.dump(data, f, indent=4)
        except Exception as e:
            print(f"Error saving JSON combination: {str(e)}")

    def _generate_sample_params(self, delta_E_max_dict: Dict[str, float]) -> List[Tuple[float, float]]:
        """Generate sample parameters with balanced temperature distribution."""
        num_temperatures = len(self.temperature_range)
        samples_per_temp = self.num_samples // num_temperatures
        remainder = self.num_samples % num_temperatures

        temp_specific_params = {T: [] for T in self.temperature_range}
        for i, T in enumerate(self.temperature_range):
            n_samples_for_temp = samples_per_temp + (1 if i < remainder else 0)
            delta_E_values = [v for v in delta_E_max_dict.values() if v > 0]
            delta_E_max_value = max(delta_E_values) if delta_E_values else 4.0
            n_e_min = self._calculate_lte_electron_density(T, delta_E_max_value)
            valid_n_e = [n_e for n_e in self.electron_density_range if n_e >= n_e_min]
            if not valid_n_e:
                print(f"Warning: No valid electron densities for T={T} K. Using full range.")
                valid_n_e = self.electron_density_range

            max_ne_per_temp = min(len(valid_n_e), 5)
            if max_ne_per_temp == 0:
                max_ne_per_temp = len(self.electron_density_range)
            selected_n_e = np.random.choice(valid_n_e, size=max_ne_per_temp, replace=False).tolist()

            if len(selected_n_e) < n_samples_for_temp:
                additional_n_e = np.random.choice(selected_n_e, size=n_samples_for_temp - len(selected_n_e), replace=True)
                selected_n_e.extend(additional_n_e)

            temp_specific_params[T].extend([(T, n_e) for n_e in selected_n_e[:n_samples_for_temp]])

        # Round-robin selection for even distribution
        sample_params = []
        indices = {T: 0 for T in self.temperature_range}
        remaining = {T: len(params) for T, params in temp_specific_params.items()}
        total_remaining = sum(remaining.values())

        while total_remaining > 0:
            for T in self.temperature_range:
                if remaining[T] > 0:
                    idx = indices[T]
                    sample_params.append(temp_specific_params[T][idx])
                    indices[T] += 1
                    remaining[T] -= 1
                    total_remaining -= 1

        if len(sample_params) < self.num_samples:
            print(f"Warning: Only {len(sample_params)} samples generated, duplicating...")
            while len(sample_params) < self.num_samples:
                idx = np.random.randint(0, len(sample_params))
                sample_params.append(sample_params[idx])
        elif len(sample_params) > self.num_samples:
            sample_params = sample_params[:self.num_samples]

        return sample_params

    def generate_dataset(
        self,
        simulators: List[SpectrumSimulator],
        delta_E_max: Dict[str, float],
        ionization_energies: Dict[str, float],
        processed_dir: str,
        drive_processed_dir: str
    ) -> None:
        """Generate and save dataset to HDF5, appending to train/validation/test subgroups."""
        # Set seed for consistency
        np.random.seed(42)
        self.combinations_json_path = os.path.join(drive_processed_dir, "combinations.json")
        self._load_used_combinations()
        mixed_simulator = MixedSpectrumSimulator(simulators, self.config, delta_E_max)
        spectra_list, labels_list, wavelengths_list, atom_percentages_list = [], [], [], []

        sample_params = self._generate_sample_params(delta_E_max)

        # Debug final distribution
        temps = [param[0] for param in sample_params]
        temp_counts = Counter(temps)
        for temp, count in temp_counts.items():
            print(f"Final distribution - Temperature {temp} K: {count} samples ({count/self.num_samples*100:.2f}%)")


        for i, (T, n_e) in enumerate(tqdm(sample_params, desc="Generating dataset")):
            mixed_simulator.current_T = T
            mixed_simulator.current_n_e = n_e
            max_attempts = 5
            attempt = 0
            sample_generated = False

            while attempt < max_attempts and not sample_generated:
                result = mixed_simulator.generate_sample(ionization_energies)
                if result[0] is None:
                    attempt += 1
                    continue

                wavelengths, spectrum, labels, atom_percentages = result
                combination_hash = self._hash_combination(T, n_e, atom_percentages)
                if combination_hash in self.used_combinations:
                    print(f"Combination already exists for T={T} K, n_e={n_e:.2e} cm^-3, retrying ({attempt + 1}/{max_attempts})")
                    attempt += 1
                    continue

                spectra_list.append(spectrum)
                labels_list.append(labels)
                wavelengths_list.append(wavelengths)
                atom_percentages_list.append(atom_percentages)

                # Save combination to JSON
                combination = {
                    'sample_id': f"sample_{len(self.used_combinations) + 1}",
                    'hash': combination_hash,
                    'temperature': float(T),
                    'electron_density': float(n_e),
                    'elements': {k: v for k, v in atom_percentages.items()
                                if k not in ['temperature', 'electron_density', 'delta_E_max']},
                    'delta_E_max': atom_percentages['delta_E_max']
                }
                self._save_combination(combination)
                self.used_combinations.add(combination_hash)
                sample_generated = True

            if not sample_generated:
                print(f"Warning: Failed to generate unique sample for T={T} K, n_e={n_e:.2e} cm^-3 after {max_attempts} attempts")


            if (i + 1) % (self.num_samples // 10) == 0:
                current_temps = [param[0] for param in sample_params[:i + 1]]
                current_counts = Counter(current_temps)
                print(f"Progress {(i + 1)/self.num_samples*100:.1f}%:")
                for temp, count in current_counts.items():
                    print(f"Temperature {temp} K: {count} samples ({count/(i + 1)*100:.2f}%)")


        if not spectra_list:
            raise ValueError("No valid samples generated. Check NIST data or simulator configuration.")

        spectra_array = np.array(spectra_list, dtype=np.float32)
        labels_array = np.array(labels_list, dtype=np.int32)
        wavelengths_array = np.array(wavelengths_list[0], dtype=np.float32)  # Take the first wavelengths (should be same)
        atom_percentages_array = [json.dumps(d).encode('utf-8') for d in atom_percentages_list]

        # Split data into train, validation, and test
        actual_samples = len(spectra_list)
        num_train = int(0.7 * actual_samples)
        num_val = int(0.15 * actual_samples)
        num_test = actual_samples - num_train - num_val

        indices = np.random.permutation(actual_samples)
        train_idx = indices[:num_train]
        val_idx = indices[num_train:num_train + num_val]
        test_idx = indices[num_train + num_val:]

        train_data = (
            spectra_array[train_idx],
            labels_array[train_idx],
            [atom_percentages_array[i] for i in train_idx]
        )
        val_data = (
            spectra_array[val_idx],
            labels_array[val_idx],
            [atom_percentages_array[i] for i in val_idx]
        )
        test_data = (
            spectra_array[test_idx],
            labels_array[test_idx],
            [atom_percentages_array[i] for i in test_idx]
        )

        # Save to HDF5 in drive_processed_dir
        output_filename = "spectral_dataset.h5"
        drive_output_path = os.path.join(drive_processed_dir, output_filename)
        os.makedirs(drive_processed_dir, exist_ok=True)

        # Backup if file exists
        if os.path.exists(drive_output_path):
            backup_path = os.path.join(drive_processed_dir, f"spectral_dataset_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.h5")
            shutil.copy(drive_output_path, backup_path)
            print(f"Backup created at {backup_path}")

        # Read existing data (if any)
        existing_data = {'train': {'spectra': [], 'labels': [], 'atom_percentages': []},
                        'validation': {'spectra': [], 'labels': [], 'atom_percentages': []},
                        'test': {'spectra': [], 'labels': [], 'atom_percentages': []}}
        total_existing_samples = 0

        if os.path.exists(drive_output_path):
            with h5py.File(drive_output_path, 'r') as f:
                # Validate wavelengths if present
                if 'wavelengths' in f:
                    existing_wavelengths = f['wavelengths'][:]
                    if not np.array_equal(existing_wavelengths, wavelengths_array):
                        raise ValueError("New wavelengths do not match existing wavelengths")

                for ds_name in ['train', 'validation', 'test']:
                    if ds_name in f:
                        if 'spectra' in f[ds_name]:
                            existing_data[ds_name]['spectra'] = f[ds_name]['spectra'][:]
                        if 'labels' in f[ds_name]:
                            existing_data[ds_name]['labels'] = f[ds_name]['labels'][:]
                        if 'atom_percentages' in f[ds_name]:
                            existing_data[ds_name]['atom_percentages'] = f[ds_name]['atom_percentages'][:]
                        total_existing_samples += len(existing_data[ds_name]['spectra'])

        # Combine new data with old data
        combined_data = {}
        for ds_name, new_data in [('train', train_data), ('validation', val_data), ('test', test_data)]:
            new_spectra, new_labels, new_atom_percentages = new_data
            existing_spectra = existing_data[ds_name]['spectra']
            existing_labels = existing_data[ds_name]['labels']
            existing_atom_percentages = existing_data[ds_name]['atom_percentages']

            combined_spectra = np.concatenate([existing_spectra, new_spectra]) if len(existing_spectra) > 0 else new_spectra
            combined_labels = np.concatenate([existing_labels, new_labels]) if len(existing_labels) > 0 else new_labels
            existing_atom_percentages_list = existing_atom_percentages.tolist() if isinstance(existing_atom_percentages, np.ndarray) else existing_atom_percentages
            combined_atom_percentages = existing_atom_percentages_list + new_atom_percentages if len(existing_atom_percentages_list) > 0 else new_atom_percentages

            combined_data[ds_name] = (combined_spectra, combined_labels, combined_atom_percentages)

        # Save to HDF5 with 'a' mode (append)
        with h5py.File(drive_output_path, 'a') as f:
            # Delete old subgroups if they exist
            for ds_name in ['train', 'validation', 'test']:
                if ds_name in f:
                    del f[ds_name]

            # Save wavelengths in root
            if 'wavelengths' in f:
                del f['wavelengths']
            f.create_dataset('wavelengths', data=wavelengths_array)

            # Save combined data
            total_samples = 0
            for ds_name, (spectra, labels, atom_percentages) in combined_data.items():
                ds_grp = f.create_group(ds_name)
                ds_grp.create_dataset('spectra', data=spectra, compression='gzip')
                ds_grp.create_dataset('labels', data=labels, compression='gzip')
                ds_grp.create_dataset('atom_percentages', data=atom_percentages, compression='gzip')
                total_samples += len(spectra)

            # Update root attributes
            f.attrs['last_updated'] = datetime.now().isoformat()
            f.attrs['total_samples'] = total_samples
            f.attrs['simulation_config'] = json.dumps(self.config)

        print(f"Dataset appended to {drive_output_path} in train/validation/test subgroups")
        print(f"Total samples: {total_samples} (new: {actual_samples}, old: {total_existing_samples})")


class DataManager:
    """Manages data loading, validation, and file operations."""

    def __init__(self, drive_base_dir: str):
        self.drive_base_dir = drive_base_dir
        self.data_dir = os.path.join(drive_base_dir, "data/raw/HDF5")
        self.processed_dir = "/content"
        self.drive_processed_dir = os.path.join(drive_base_dir, "data/processed")
        self.nist_source_path = os.path.join(self.data_dir, "nist_data(1).h5")
        self.nist_target_path = "/content/nist_data(1).h5"
        self.atomic_data_source_path = os.path.join(self.data_dir, "atomic_data1.h5")
        self.atomic_data_target_path = "/content/atomic_data1.h5"
        self.json_map_path = os.path.join(self.drive_base_dir, "data/processed", "element_map.json")

    def mount_drive(self) -> None:
        """Mount Google Drive."""
        mountpoint = '/content/drive'
        try:
            drive.flush_and_unmount()
        except ValueError:
            pass
        os.makedirs(mountpoint, exist_ok=True)
        drive.mount(mountpoint, force_remount=True)

    def copy_files(self) -> None:
        """Copies necessary files from Drive to local."""
        for source, target in tqdm([(self.nist_source_path, self.nist_target_path),
                                  (self.atomic_data_source_path, self.atomic_data_target_path)],
                                 desc="Copying files"):
            if not os.path.exists(source):
                raise FileNotFoundError(f"File not found at {source}")
            shutil.copy(source, target)

    def load_element_map(self) -> Dict:
        """Loads and validates the element map."""
        if not os.path.exists(self.json_map_path):
            raise FileNotFoundError(f"element_map.json not found at {self.json_map_path}")
        with open(self.json_map_path, 'r') as f:
            element_map = json.load(f)

        for elem in REQUIRED_ELEMENTS:
            if elem not in element_map or not isinstance(element_map[elem], list) or abs(sum(element_map[elem]) - 1.0) > 1e-6:
                raise ValueError(f"Invalid element_map for {elem}: must be a list summing to 1.0")

        return element_map

    def load_ionization_energies(self) -> Dict[str, float]:
        """Loads ionization energies from atomic data."""
        ionization_energies = {}
        try:
            with h5py.File(self.atomic_data_target_path, 'r') as f:
                dset = f['elements']
                columns = dset.attrs.get('columns', ['At. num', 'Sp. Name', 'Ion Charge', 'El. Name',
                                                   'Prefix', 'Ionization Energy (eV)', 'Suffix'])
                if 'columns' not in dset.attrs:
                    print("Warning: 'columns' attribute missing. Using default columns.")

                data = [[item[0], item[1].decode('utf-8'), item[2].decode('utf-8'), item[3].decode('utf-8'),
                        item[4].decode('utf-8'), item[5], item[6].decode('utf-8')] for item in dset[:]]
                df_ionization = pd.DataFrame(data, columns=columns)

                species_col = ion_energy_col = None
                for col in columns:
                    if col.lower() in ['sp.', 'species', 'sp', 'element', 'sp. name']:
                        species_col = col
                    if 'ionization' in col.lower() and 'ev' in col.lower():
                        ion_energy_col = col

                if not species_col or not ion_energy_col:
                    raise KeyError(f"Required columns not found. Available: {list(df_ionization.columns)}")

                for _, row in df_ionization.iterrows():
                    try:
                        ionization_energies[row[species_col]] = float(row[ion_energy_col])
                    except (ValueError, TypeError):
                        print(f"Warning: Invalid ionization energy for {row[species_col]}, using 0.0 eV")
                        ionization_energies[row[species_col]] = 0.0

        except Exception as e:
            print(f"Error loading atomic_data1.h5: {str(e)}")
            raise

        for elem in REQUIRED_ELEMENTS:
            base_elem, ion = elem.split('_')
            ion_level = 'I' if ion == '1' else 'II'
            sp_name = f"{base_elem} {ion_level}"
            if sp_name not in ionization_energies:
                print(f"Warning: No ionization energy for {sp_name}, using 0.0 eV")
                ionization_energies[sp_name] = 0.0

        return ionization_energies


def main():
    """Main function to run simulation and generate dataset."""
    pd.set_option('future.no_silent_downcasting', True)

    data_manager = DataManager("/content/drive/MyDrive/libs_lstm")

    # Mount Drive and copy files
    data_manager.mount_drive()
    data_manager.copy_files()

    # Load element map and ionization energies
    element_map = data_manager.load_element_map()
    ionization_energies = data_manager.load_ionization_energies()

    # Load NIST data
    fetcher = DataFetcher(data_manager.nist_target_path)
    nist_data_dict = {}
    delta_E_max_dict = {}
    for elem in REQUIRED_ELEMENTS:
        element, ion = elem.split('_')
        data, delta_E = fetcher.get_nist_data(element, int(ion))
        nist_data_dict[elem] = data
        delta_E_max_dict[elem] = delta_E
        if not data:
            print(f"No NIST data for {elem}")

    # Create simulators
    simulators = []
    for elem in nist_data_dict:
        if nist_data_dict[elem]:
            element, ion = elem.split('_')
            ion_energy = ionization_energies.get(f"{element} {'I' if int(ion) == 1 else 'II'}", 0.0)
            simulator = SpectrumSimulator(
                nist_data_dict[elem],
                element,
                int(ion),
                SIMULATION_CONFIG["temperature_range"],
                ion_energy,
                SIMULATION_CONFIG
            )
            simulators.append(simulator)

    if not simulators:
        raise ValueError("No valid simulators created. Check NIST data.")

    # Generate dataset
    generator = DatasetGenerator(SIMULATION_CONFIG)
    generator.generate_dataset(
        simulators,
        delta_E_max_dict,
        ionization_energies,
        data_manager.processed_dir,
        data_manager.drive_processed_dir
    )


if __name__ == "__main__":
    main()